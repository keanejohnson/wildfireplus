{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## VARIABLES ##\n",
    "###############\n",
    "\n",
    "# name of directory with fire tif files\n",
    "tif_directory = \"toydata\"\n",
    "\n",
    "# name of directory with weather data\n",
    "weather_directory = 'weather_data'\n",
    "\n",
    "#####################\n",
    "## HYPERPARAMETERS ##\n",
    "#####################\n",
    "# scale the weather data - yea or nay\n",
    "scaled_weather = False\n",
    "\n",
    "# the desired height and width (in pixels) of the matrix to feed into the CNN\n",
    "# 1 pixel side = 500 meters = 0.310686 miles\n",
    "matrix_dim = 32\n",
    "\n",
    "# multiplier for amount of zero-labeled data we want to add to dataset\n",
    "labeled_multiplier = 1\n",
    "\n",
    "# test size for train/test split\n",
    "test_size = 0.2\n",
    "\n",
    "# training epochs\n",
    "epoc = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Dataset Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(directory):\n",
    "    '''\n",
    "    Process the dataset in the supplied directory and return matrices of which pixels belong to which fire and \n",
    "    which day of the year the pixel was on fire.\n",
    "    \n",
    "    Args: \n",
    "        - directory: name of directory with tif files\n",
    "    Returns: \n",
    "        - fire_data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "\n",
    "    tiff_files = []\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.tif'):\n",
    "            tiff_files.append(path + '/' + f)\n",
    "\n",
    "    tiff_dict = {}\n",
    "\n",
    "    # dictionary of tiff files\n",
    "    for f in tiff_files:\n",
    "        k = f.split('/')[-1].split('.tif')[0]\n",
    "        tiff_dict[k] = f\n",
    "\n",
    "    # convert to np array\n",
    "    fire_id = Image.open(tiff_dict['fireid'])\n",
    "    fire_id = np.array(fire_id)\n",
    "    fire_id[fire_id == -9999] = 0\n",
    "\n",
    "    fireline = Image.open(tiff_dict['Global_fire_atlas_firelinecrop'])\n",
    "    fireline = np.array(fireline)\n",
    "    fireline[fireline == -9999] = 0\n",
    "\n",
    "    # get list of unique fire_ids\n",
    "    fire_ids = set()\n",
    "\n",
    "    for row in fire_id:\n",
    "        for val in row:\n",
    "            fire_ids.add(val)\n",
    "\n",
    "    # remove 0 from fire_ids set because it does not denote a fire\n",
    "    fire_ids.remove(0)\n",
    "\n",
    "    # get dict with key value pairs of fire_id and an empty dict\n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        id = str(id)\n",
    "        fire_data_dict[id] = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        indices = np.where(fire_id == id, 1, 0)\n",
    "        fire_data_dict[str(id)] = indices\n",
    "        \n",
    "    return fire_data_dict, fireline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_matrices(data_dict, fireline):\n",
    "    '''\n",
    "    Create matrices for each fire_id that show were the fire was on a given day during the year.\n",
    "    \n",
    "    Args:\n",
    "        - data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    Returns:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    '''\n",
    "    \n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for key, val in data_dict.items():\n",
    "        data = {}\n",
    "                \n",
    "        for y in range(1, 366):\n",
    "            mask = ((fireline == y) & (val == 1))\n",
    "            mask = mask.astype(int)\n",
    "        \n",
    "            if np.sum(mask) > 0:\n",
    "                data[str(y)] = mask\n",
    "        \n",
    "        fire_data_dict[key] = data\n",
    "        \n",
    "    return fire_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_day_pairs(fire_data_dict):\n",
    "    '''\n",
    "    Create a list of sets where the first value is where the fire was on a given day and the second value is where\n",
    "    the fire was on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    Returns:\n",
    "        - train_labels: a list of sets where the first value of the set is a one-hot encoded 2D array of fire \n",
    "        spread on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2:\n",
    "        [\n",
    "            (one-hot encoded 2D array of fire spread on that day_1, one-hot encoded 2D array of fire spread on day_2),\n",
    "            (one-hot encoded 2D array of fire spread on that day_2, one-hot encoded 2D array of fire spread on day_3),\n",
    "        ]\n",
    "    '''\n",
    "    \n",
    "    train_labels = []\n",
    "\n",
    "    for key, value in fire_data_dict.items():\n",
    "        burn_matrices = list(value.values())\n",
    "        day_of_year = list(value.keys())\n",
    "        \n",
    "        for index, day in enumerate(burn_matrices):\n",
    "\n",
    "            if index < len(burn_matrices) - 1:\n",
    "                day_1 = burn_matrices[index]\n",
    "                day_2_index = index + 1\n",
    "                day_2 = burn_matrices[day_2_index]\n",
    "                \n",
    "                doy = day_of_year[day_2_index]\n",
    "                \n",
    "                pair = (day_1, day_2)\n",
    "                train_labels.append((doy, pair))\n",
    "\n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_dict(directory, scaled_weather):\n",
    "    '''\n",
    "    Create a dictionary of weather data from a pickled file\n",
    "    Args:\n",
    "        - directory: path to weather pickle file\n",
    "        - scaled_weather: True/False to scale using max value\n",
    "    Returns:\n",
    "        - weather_data: dictionary of key (day of year) and value (dictionary of key (weather parameter) \n",
    "        and value (matrix of value for each pixel))\n",
    "    '''\n",
    "\n",
    "    path = os.path.abspath(directory)\n",
    "    \n",
    "    weather_file = ''\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.pickle'):\n",
    "            weather_file = path + '/' + f\n",
    "    \n",
    "    weather = pd.read_pickle(weather_file)\n",
    "    \n",
    "    weather_dict = {}\n",
    "    \n",
    "    for k, v in weather.items():\n",
    "        weather_dict[k] = {}\n",
    "        \n",
    "        for att, matrix in v.items():\n",
    "            mat = np.nan_to_num(matrix)\n",
    "            weather_dict[k][att] = mat\n",
    "     \n",
    "    weather_data = {}\n",
    "\n",
    "    for k, v in weather_dict.items():\n",
    "        doy = dt.strptime(k, \"%Y-%m-%d\").strftime(\"%j\")\n",
    "        weather_data[doy] = v\n",
    "    \n",
    "    # scale weather data\n",
    "    vals = list(weather_data.values())[0]\n",
    "    weather_atts = list(vals.keys())\n",
    "    max_values = dict.fromkeys(weather_atts, 0)\n",
    "    \n",
    "    if scaled_weather == True:\n",
    "        \n",
    "        for k, v in weather_dict.items():\n",
    "\n",
    "            for weather_att, matrix in v.items():\n",
    "                max_val = matrix.max()\n",
    "                if max_val > max_values[weather_att]:\n",
    "                    max_values[weather_att] = max_val\n",
    "    \n",
    "    return weather_data, max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(max_values, scaled_weather, day_of_year, x, y):\n",
    "    '''\n",
    "    Fetch weather data for the relevant day and pixel.\n",
    "    \n",
    "    Args:\n",
    "        - max_values: list of max_values for each weather features\n",
    "        - scaled_weather: whether the weather data should be scaled - true/false\n",
    "        - day_of_year: day of the year (1-365)\n",
    "        - x: x-coordinate of matrix\n",
    "        - y: y-coordinate of matrix\n",
    "    Returns:\n",
    "        - weather_list: an array of relevant weather data for that pixel\n",
    "    '''\n",
    "    weather_list = []\n",
    "    \n",
    "    day_weather = weather_data.get(day_of_year)\n",
    "\n",
    "    if day_weather is None:\n",
    "        return None\n",
    "    else:\n",
    "        for k, v in day_weather.items():\n",
    "            if scaled_weather == True:\n",
    "                max_val = max_values.get(k, 1)\n",
    "                \n",
    "                try:\n",
    "                    val = v[x,y]/max_val\n",
    "                    value = val/max_val\n",
    "                    \n",
    "                    if math.isnan(value):\n",
    "                        weather_list.append(0)\n",
    "                    else:\n",
    "                        weather_list.append(value)\n",
    "                except IndexError:\n",
    "                    return None\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    weather_list.append(v[x,y])\n",
    "                except IndexError:\n",
    "                    return None\n",
    "    \n",
    "    return weather_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Dataset for CNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset, matrix_dim, num_pixels, side):\n",
    "    '''\n",
    "    Supplement the list produced in `create_labeled_data` with data where there was no data\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "        - num_pixels: how many \"no-fire\" pixel-matrix pairs we want to return\n",
    "        - side: half the length of the dimension of the outpur matrix\n",
    "    Returns:\n",
    "        - no_fire: a list of sets, where the second value (0, 1) represents whether fire is present for a given pixel, and the\n",
    "        first value is a matrix centered on the second value for the previous day and represents where the fire was\n",
    "        on the previous day\n",
    "    '''\n",
    "        \n",
    "    no_fire = []\n",
    "\n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 0)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "        for (xi, yi) in vals:\n",
    "            xi_r = xi + side\n",
    "            xi_l = xi - side\n",
    "            yi_b = yi + side\n",
    "            yi_t = yi - side\n",
    "\n",
    "            m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "                                    \n",
    "            # control for edge cases where shape doesn't match up - not sure why this is happening\n",
    "            if m.shape == (matrix_dim, matrix_dim):\n",
    "                weather_data = fetch_weather_data(max_values, scaled_weather, doy, xi, yi)\n",
    "                if weather_data is not None:\n",
    "                    no_fire.append(((weather_data, m), 0))\n",
    "    \n",
    "    no_fire = random.sample(no_fire, num_pixels)\n",
    "    \n",
    "    return no_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_data(dataset, matrix_dim):\n",
    "    '''\n",
    "    Create a list of sets where the first value is a matrix of pixels on a given day and the second value denotes\n",
    "    whether there was fire in the center pixel on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "    Returns:\n",
    "        - data: a list of sets, where the second value (0, 1) represents whether fire is present for a given pixel, and the\n",
    "        first value is a matrix centered on the second value for the previous day and represents where the fire was\n",
    "        on the previous day\n",
    "    '''\n",
    "\n",
    "    side = int(matrix_dim/2)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 1)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "        for (xi, yi) in vals:\n",
    "            xi_r = xi + side\n",
    "            xi_l = xi - side\n",
    "            yi_b = yi + side\n",
    "            yi_t = yi - side\n",
    "\n",
    "            m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "                        \n",
    "            weather_data = fetch_weather_data(max_values, scaled_weather, doy, xi, yi)\n",
    "            \n",
    "            if weather_data is not None:\n",
    "                data.append(((weather_data, m), 1))\n",
    "    \n",
    "    data_len = len(data)\n",
    "    num_pixels = min(int(data_len*labeled_multiplier), data_len)\n",
    "    \n",
    "    # balance this dataset with values where there is no fire\n",
    "    no_fire = balance_dataset(dataset, matrix_dim, num_pixels, side)\n",
    "    \n",
    "    # combine and shuffle\n",
    "    data += no_fire    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset_for_cnn(data):\n",
    "    '''\n",
    "    Takes a list of ((list, matrix), integer) pairs and returns fire data, weather data, and output labels \n",
    "    split into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        - data: a list of (matrix, integer) pairs\n",
    "    Returns:\n",
    "        - fire: array of input data in matrix_dim X matrix_dim shape\n",
    "        - weather: list of scaled weather weights\n",
    "        - Y: array of output labels (0 or 1)\n",
    "    '''\n",
    "    \n",
    "    fire = []\n",
    "    weather = []\n",
    "    Y = []\n",
    "\n",
    "    for ((w, f), y) in data:\n",
    "        f = np.asarray(f)\n",
    "        fire.append(f)\n",
    "        \n",
    "        w = np.asarray(w)\n",
    "        weather.append(w)\n",
    "        \n",
    "        Y.append(y)\n",
    "\n",
    "    fire = np.asarray(fire)\n",
    "    weather = np.asarray(weather)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    obs = len(fire)\n",
    "    \n",
    "    fire = fire.reshape(obs, matrix_dim, matrix_dim, 1)\n",
    "\n",
    "    return fire, weather, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data, max_values = create_weather_dict(weather_directory, scaled_weather)\n",
    "fire_data_dict, fireline = data_processing(tif_directory)\n",
    "fire_data_dict = create_one_hot_matrices(fire_data_dict, fireline)\n",
    "small_dataset = create_day_pairs(fire_data_dict)\n",
    "data = create_labeled_data(small_dataset, matrix_dim)\n",
    "fire, weather, Y = prep_dataset_for_cnn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import AveragePooling2D, Conv1D, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1 score manually\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_pos / (possible_pos + K.epsilon())\n",
    "\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_pos / (predicted_pos + K.epsilon())\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    score = 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Fire Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_1: fire image data with Sequential API\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model_1.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid'))\n",
    "model_1.add(Conv2D(64, kernel_size=(3, 3), activation='sigmoid'))\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# Final dense layer \n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_1.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy', f1_score, tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5409 samples, validate on 1353 samples\n",
      "Epoch 1/30\n",
      "5409/5409 [==============================] - 2s 331us/step - loss: 0.6979 - accuracy: 0.5062 - f1_score: 0.4614 - auc_49: 0.5047 - val_loss: 0.6934 - val_accuracy: 0.4945 - val_f1_score: 0.0000e+00 - val_auc_49: 0.5176\n",
      "Epoch 2/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.6271 - accuracy: 0.6293 - f1_score: 0.6010 - auc_49: 0.5421 - val_loss: 0.4840 - val_accuracy: 0.5055 - val_f1_score: 0.6691 - val_auc_49: 0.6048\n",
      "Epoch 3/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.2813 - accuracy: 0.9018 - f1_score: 0.8895 - auc_49: 0.6925 - val_loss: 0.4235 - val_accuracy: 0.5055 - val_f1_score: 0.6691 - val_auc_49: 0.7560\n",
      "Epoch 4/30\n",
      "5409/5409 [==============================] - 2s 286us/step - loss: 0.1947 - accuracy: 0.9327 - f1_score: 0.9212 - auc_49: 0.7938 - val_loss: 0.1720 - val_accuracy: 0.9808 - val_f1_score: 0.9797 - val_auc_49: 0.8350\n",
      "Epoch 5/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1579 - accuracy: 0.9523 - f1_score: 0.9482 - auc_49: 0.8621 - val_loss: 0.1201 - val_accuracy: 0.9660 - val_f1_score: 0.9643 - val_auc_49: 0.8835\n",
      "Epoch 6/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1297 - accuracy: 0.9604 - f1_score: 0.9521 - auc_49: 0.8990 - val_loss: 0.1018 - val_accuracy: 0.9593 - val_f1_score: 0.9561 - val_auc_49: 0.9120\n",
      "Epoch 7/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.1164 - accuracy: 0.9651 - f1_score: 0.9622 - auc_49: 0.9218 - val_loss: 0.0942 - val_accuracy: 0.9845 - val_f1_score: 0.9838 - val_auc_49: 0.9304\n",
      "Epoch 8/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1087 - accuracy: 0.9700 - f1_score: 0.9686 - auc_49: 0.9372 - val_loss: 0.0766 - val_accuracy: 0.9837 - val_f1_score: 0.9831 - val_auc_49: 0.9432\n",
      "Epoch 9/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.1057 - accuracy: 0.9725 - f1_score: 0.9648 - auc_49: 0.9480 - val_loss: 0.0761 - val_accuracy: 0.9727 - val_f1_score: 0.9712 - val_auc_49: 0.9522\n",
      "Epoch 10/30\n",
      "5409/5409 [==============================] - 1s 272us/step - loss: 0.0934 - accuracy: 0.9750 - f1_score: 0.9737 - auc_49: 0.9559 - val_loss: 0.0696 - val_accuracy: 0.9778 - val_f1_score: 0.9768 - val_auc_49: 0.9591\n",
      "Epoch 11/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.0944 - accuracy: 0.9763 - f1_score: 0.9744 - auc_49: 0.9617 - val_loss: 0.0692 - val_accuracy: 0.9778 - val_f1_score: 0.9770 - val_auc_49: 0.9642\n",
      "Epoch 12/30\n",
      "5409/5409 [==============================] - 2s 288us/step - loss: 0.0846 - accuracy: 0.9802 - f1_score: 0.9797 - auc_49: 0.9664 - val_loss: 0.0555 - val_accuracy: 0.9860 - val_f1_score: 0.9855 - val_auc_49: 0.9685\n",
      "Epoch 13/30\n",
      "5409/5409 [==============================] - 2s 290us/step - loss: 0.0836 - accuracy: 0.9791 - f1_score: 0.9728 - auc_49: 0.9702 - val_loss: 0.0574 - val_accuracy: 0.9926 - val_f1_score: 0.9925 - val_auc_49: 0.9719\n",
      "Epoch 14/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0768 - accuracy: 0.9845 - f1_score: 0.9839 - auc_49: 0.9733 - val_loss: 0.0848 - val_accuracy: 0.9956 - val_f1_score: 0.9954 - val_auc_49: 0.9747\n",
      "Epoch 15/30\n",
      "5409/5409 [==============================] - 2s 284us/step - loss: 0.0777 - accuracy: 0.9821 - f1_score: 0.9758 - auc_49: 0.9757 - val_loss: 0.0517 - val_accuracy: 0.9852 - val_f1_score: 0.9848 - val_auc_49: 0.9769\n",
      "Epoch 16/30\n",
      "5409/5409 [==============================] - 2s 299us/step - loss: 0.0705 - accuracy: 0.9854 - f1_score: 0.9841 - auc_49: 0.9779 - val_loss: 0.0453 - val_accuracy: 0.9911 - val_f1_score: 0.9911 - val_auc_49: 0.9789\n",
      "Epoch 17/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0728 - accuracy: 0.9828 - f1_score: 0.9822 - auc_49: 0.9798 - val_loss: 0.0483 - val_accuracy: 0.9941 - val_f1_score: 0.9941 - val_auc_49: 0.9806\n",
      "Epoch 18/30\n",
      "5409/5409 [==============================] - 2s 288us/step - loss: 0.0704 - accuracy: 0.9852 - f1_score: 0.9844 - auc_49: 0.9813 - val_loss: 0.0529 - val_accuracy: 0.9860 - val_f1_score: 0.9859 - val_auc_49: 0.9820\n",
      "Epoch 19/30\n",
      "5409/5409 [==============================] - 2s 285us/step - loss: 0.0670 - accuracy: 0.9858 - f1_score: 0.9839 - auc_49: 0.9826 - val_loss: 0.0764 - val_accuracy: 0.9956 - val_f1_score: 0.9954 - val_auc_49: 0.9831\n",
      "Epoch 20/30\n",
      "5409/5409 [==============================] - 2s 311us/step - loss: 0.0657 - accuracy: 0.9869 - f1_score: 0.9865 - auc_49: 0.9837 - val_loss: 0.0415 - val_accuracy: 0.9889 - val_f1_score: 0.9890 - val_auc_49: 0.9842\n",
      "Epoch 21/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0643 - accuracy: 0.9852 - f1_score: 0.9787 - auc_49: 0.9847 - val_loss: 0.0399 - val_accuracy: 0.9941 - val_f1_score: 0.9941 - val_auc_49: 0.9851\n",
      "Epoch 22/30\n",
      "5409/5409 [==============================] - 2s 283us/step - loss: 0.0657 - accuracy: 0.9869 - f1_score: 0.9866 - auc_49: 0.9856 - val_loss: 0.0387 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9860\n",
      "Epoch 23/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0564 - accuracy: 0.9878 - f1_score: 0.9817 - auc_49: 0.9864 - val_loss: 0.0374 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9867\n",
      "Epoch 24/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.0571 - accuracy: 0.9880 - f1_score: 0.9878 - auc_49: 0.9871 - val_loss: 0.0414 - val_accuracy: 0.9897 - val_f1_score: 0.9896 - val_auc_49: 0.9874\n",
      "Epoch 25/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0573 - accuracy: 0.9869 - f1_score: 0.9806 - auc_49: 0.9878 - val_loss: 0.0401 - val_accuracy: 0.9897 - val_f1_score: 0.9897 - val_auc_49: 0.9880\n",
      "Epoch 26/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0523 - accuracy: 0.9893 - f1_score: 0.9890 - auc_49: 0.9883 - val_loss: 0.0371 - val_accuracy: 0.9904 - val_f1_score: 0.9905 - val_auc_49: 0.9886\n",
      "Epoch 27/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0530 - accuracy: 0.9889 - f1_score: 0.9881 - auc_49: 0.9889 - val_loss: 0.0371 - val_accuracy: 0.9911 - val_f1_score: 0.9912 - val_auc_49: 0.9891\n",
      "Epoch 28/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0516 - accuracy: 0.9882 - f1_score: 0.9875 - auc_49: 0.9893 - val_loss: 0.0340 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9896\n",
      "Epoch 29/30\n",
      "5409/5409 [==============================] - 2s 285us/step - loss: 0.0505 - accuracy: 0.9887 - f1_score: 0.9822 - auc_49: 0.9898 - val_loss: 0.0389 - val_accuracy: 0.9889 - val_f1_score: 0.9892 - val_auc_49: 0.9900\n",
      "Epoch 30/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0508 - accuracy: 0.9889 - f1_score: 0.9885 - auc_49: 0.9902 - val_loss: 0.0348 - val_accuracy: 0.9919 - val_f1_score: 0.9921 - val_auc_49: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f89b7996b50>"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_1.fit(\n",
    "    x = fire, \n",
    "    y = Y,\n",
    "    validation_split = test_size, \n",
    "    epochs=epoc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00762875],\n",
       "       [1.        ],\n",
       "       [0.00762875],\n",
       "       [0.99789375],\n",
       "       [0.00762875],\n",
       "       [0.00762875],\n",
       "       [0.00762875],\n",
       "       [0.9999999 ],\n",
       "       [0.00762875],\n",
       "       [0.99999976]], dtype=float32)"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.predict(fire[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Fire Image Data and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_2: image data and weather data with functional API\n",
    "\n",
    "# Define image inputs shape\n",
    "image_shape = fire[0].shape\n",
    "image_inputs = Input(shape = image_shape)\n",
    "\n",
    "# Define weather inputs shape\n",
    "weather_shape = weather[0].shape\n",
    "weather_inputs = Input(shape = weather_shape)\n",
    "\n",
    "# Add layers for fire image interpretation\n",
    "fire_1 = AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid')(image_inputs)\n",
    "fire_2 = Conv2D(64, kernel_size=(3, 3), activation='sigmoid')(fire_1)\n",
    "fire_3 = Conv2D(32, kernel_size=(3, 3), activation='sigmoid')(fire_2)\n",
    "fire_4 = MaxPooling2D(pool_size=(2,2), strides=None, padding='valid')(fire_3)\n",
    "fire_5 = Dropout(0.2)(fire_4)\n",
    "fire_6 = Flatten()(fire_5)\n",
    "fire_7 = Dense(8, activation='sigmoid')(fire_6)\n",
    "\n",
    "# Combine the layers\n",
    "concat = concatenate([fire_7, weather_inputs])\n",
    "\n",
    "# Final dense layer \n",
    "predictions = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "# Define the model\n",
    "model_2 = Model(inputs=[image_inputs, weather_inputs], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy', f1_score, tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5409 samples, validate on 1353 samples\n",
      "Epoch 1/30\n",
      "5409/5409 [==============================] - 2s 336us/step - loss: 0.7016 - accuracy: 0.4962 - f1_score: 0.3579 - auc_52: 0.4682 - val_loss: 0.6867 - val_accuracy: 0.5469 - val_f1_score: 0.6806 - val_auc_52: 0.4999\n",
      "Epoch 2/30\n",
      "5409/5409 [==============================] - 1s 269us/step - loss: 0.5660 - accuracy: 0.7465 - f1_score: 0.6737 - auc_52: 0.5842 - val_loss: 0.4020 - val_accuracy: 0.8448 - val_f1_score: 0.8135 - val_auc_52: 0.7064\n",
      "Epoch 3/30\n",
      "5409/5409 [==============================] - 1s 269us/step - loss: 0.2931 - accuracy: 0.9301 - f1_score: 0.9220 - auc_52: 0.7838 - val_loss: 0.2448 - val_accuracy: 0.9217 - val_f1_score: 0.9138 - val_auc_52: 0.8398\n",
      "Epoch 4/30\n",
      "5409/5409 [==============================] - 1s 268us/step - loss: 0.2032 - accuracy: 0.9547 - f1_score: 0.9504 - auc_52: 0.8719 - val_loss: 0.1789 - val_accuracy: 0.9490 - val_f1_score: 0.9455 - val_auc_52: 0.8963\n",
      "Epoch 5/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 0.1486 - accuracy: 0.9665 - f1_score: 0.9653 - auc_52: 0.9129 - val_loss: 0.1084 - val_accuracy: 0.9860 - val_f1_score: 0.9856 - val_auc_52: 0.9271\n",
      "Epoch 6/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 0.1179 - accuracy: 0.9773 - f1_score: 0.9769 - auc_52: 0.9374 - val_loss: 0.1049 - val_accuracy: 0.9653 - val_f1_score: 0.9636 - val_auc_52: 0.9453\n",
      "Epoch 7/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 0.0995 - accuracy: 0.9798 - f1_score: 0.9789 - auc_52: 0.9511 - val_loss: 0.0677 - val_accuracy: 0.9874 - val_f1_score: 0.9874 - val_auc_52: 0.9569\n",
      "Epoch 8/30\n",
      "5409/5409 [==============================] - 1s 272us/step - loss: 0.0798 - accuracy: 0.9848 - f1_score: 0.9848 - auc_52: 0.9614 - val_loss: 0.0629 - val_accuracy: 0.9956 - val_f1_score: 0.9954 - val_auc_52: 0.9652\n",
      "Epoch 9/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.0717 - accuracy: 0.9863 - f1_score: 0.9851 - auc_52: 0.9682 - val_loss: 0.0718 - val_accuracy: 0.9837 - val_f1_score: 0.9834 - val_auc_52: 0.9708\n",
      "Epoch 10/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0721 - accuracy: 0.9837 - f1_score: 0.9828 - auc_52: 0.9727 - val_loss: 0.0418 - val_accuracy: 0.9926 - val_f1_score: 0.9925 - val_auc_52: 0.9747\n",
      "Epoch 11/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.0571 - accuracy: 0.9885 - f1_score: 0.9884 - auc_52: 0.9764 - val_loss: 0.0370 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9781\n",
      "Epoch 12/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0547 - accuracy: 0.9891 - f1_score: 0.9826 - auc_52: 0.9793 - val_loss: 0.0338 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9806\n",
      "Epoch 13/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.0530 - accuracy: 0.9885 - f1_score: 0.9820 - auc_52: 0.9816 - val_loss: 0.0515 - val_accuracy: 0.9867 - val_f1_score: 0.9867 - val_auc_52: 0.9826\n",
      "Epoch 14/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0515 - accuracy: 0.9896 - f1_score: 0.9838 - auc_52: 0.9834 - val_loss: 0.0299 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9842\n",
      "Epoch 15/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.0472 - accuracy: 0.9906 - f1_score: 0.9906 - auc_52: 0.9848 - val_loss: 0.0355 - val_accuracy: 0.9911 - val_f1_score: 0.9911 - val_auc_52: 0.9855\n",
      "Epoch 16/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0431 - accuracy: 0.9909 - f1_score: 0.9845 - auc_52: 0.9861 - val_loss: 0.0272 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9867\n",
      "Epoch 17/30\n",
      "5409/5409 [==============================] - 2s 287us/step - loss: 0.0463 - accuracy: 0.9900 - f1_score: 0.9837 - auc_52: 0.9871 - val_loss: 0.0280 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9876\n",
      "Epoch 18/30\n",
      "5409/5409 [==============================] - 2s 281us/step - loss: 0.0427 - accuracy: 0.9919 - f1_score: 0.9916 - auc_52: 0.9880 - val_loss: 0.0270 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9884\n",
      "Epoch 19/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0414 - accuracy: 0.9919 - f1_score: 0.9858 - auc_52: 0.9888 - val_loss: 0.0244 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9891\n",
      "Epoch 20/30\n",
      "5409/5409 [==============================] - 2s 286us/step - loss: 0.0395 - accuracy: 0.9921 - f1_score: 0.9856 - auc_52: 0.9894 - val_loss: 0.0233 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9897\n",
      "Epoch 21/30\n",
      "5409/5409 [==============================] - 2s 297us/step - loss: 0.0385 - accuracy: 0.9921 - f1_score: 0.9861 - auc_52: 0.9900 - val_loss: 0.0236 - val_accuracy: 0.9963 - val_f1_score: 0.9961 - val_auc_52: 0.9903\n",
      "Epoch 22/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.0383 - accuracy: 0.9921 - f1_score: 0.9918 - auc_52: 0.9905 - val_loss: 0.0369 - val_accuracy: 0.9970 - val_f1_score: 0.9970 - val_auc_52: 0.9908\n",
      "Epoch 23/30\n",
      "5409/5409 [==============================] - 2s 288us/step - loss: 0.0365 - accuracy: 0.9924 - f1_score: 0.9920 - auc_52: 0.9910 - val_loss: 0.0210 - val_accuracy: 0.9970 - val_f1_score: 0.9970 - val_auc_52: 0.9912\n",
      "Epoch 24/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.0380 - accuracy: 0.9919 - f1_score: 0.9855 - auc_52: 0.9914 - val_loss: 0.0214 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9916\n",
      "Epoch 25/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0370 - accuracy: 0.9924 - f1_score: 0.9862 - auc_52: 0.9918 - val_loss: 0.0312 - val_accuracy: 0.9926 - val_f1_score: 0.9925 - val_auc_52: 0.9919\n",
      "Epoch 26/30\n",
      "5409/5409 [==============================] - 2s 293us/step - loss: 0.0365 - accuracy: 0.9924 - f1_score: 0.9919 - auc_52: 0.9921 - val_loss: 0.0196 - val_accuracy: 0.9970 - val_f1_score: 0.9970 - val_auc_52: 0.9922\n",
      "Epoch 27/30\n",
      "5409/5409 [==============================] - 2s 287us/step - loss: 0.0341 - accuracy: 0.9933 - f1_score: 0.9875 - auc_52: 0.9924 - val_loss: 0.0328 - val_accuracy: 0.9911 - val_f1_score: 0.9912 - val_auc_52: 0.9925\n",
      "Epoch 28/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0373 - accuracy: 0.9921 - f1_score: 0.9919 - auc_52: 0.9926 - val_loss: 0.0208 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9928\n",
      "Epoch 29/30\n",
      "5409/5409 [==============================] - 2s 287us/step - loss: 0.0345 - accuracy: 0.9933 - f1_score: 0.9873 - auc_52: 0.9929 - val_loss: 0.0192 - val_accuracy: 0.9963 - val_f1_score: 0.9962 - val_auc_52: 0.9930\n",
      "Epoch 30/30\n",
      "5409/5409 [==============================] - 2s 387us/step - loss: 0.0334 - accuracy: 0.9932 - f1_score: 0.9933 - auc_52: 0.9931 - val_loss: 0.0246 - val_accuracy: 0.9956 - val_f1_score: 0.9955 - val_auc_52: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8a96dadc10>"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_2.fit(\n",
    "    x = [fire, weather], \n",
    "    y = Y,\n",
    "    validation_split = test_size, \n",
    "    epochs=epoc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.9407106e-04],\n",
       "       [9.9486130e-01],\n",
       "       [6.7150273e-04],\n",
       "       [9.9016285e-01],\n",
       "       [1.8604357e-03],\n",
       "       [1.5366743e-03],\n",
       "       [3.6032589e-03],\n",
       "       [9.9491888e-01],\n",
       "       [4.1573294e-03],\n",
       "       [9.9176675e-01]], dtype=float32)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.predict([fire[:10], weather[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
