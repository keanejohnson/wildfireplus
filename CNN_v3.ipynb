{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## VARIABLES ##\n",
    "###############\n",
    "\n",
    "# name of directory with fire tif files\n",
    "tif_directory = \"toydata\"\n",
    "\n",
    "# name of directory with weather data\n",
    "weather_directory = 'weather_data'\n",
    "\n",
    "#####################\n",
    "## HYPERPARAMETERS ##\n",
    "#####################\n",
    "# scale the weather data - yea or nay\n",
    "scaled_weather = False\n",
    "\n",
    "# the desired height and width (in pixels) of the matrix to feed into the CNN\n",
    "# 1 pixel side = 500 meters = 0.310686 miles\n",
    "matrix_dim = 32\n",
    "\n",
    "# multiplier for amount of zero-labeled data we want to add to dataset\n",
    "labeled_multiplier = 1\n",
    "\n",
    "# test size for train/test split\n",
    "test_size = 0.2\n",
    "\n",
    "# training epochs\n",
    "epoc = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Dataset Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(directory):\n",
    "    '''\n",
    "    Process the dataset in the supplied directory and return matrices of which pixels belong to which fire and \n",
    "    which day of the year the pixel was on fire.\n",
    "    \n",
    "    Args: \n",
    "        - directory: name of directory with tif files\n",
    "    Returns: \n",
    "        - fire_data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "\n",
    "    tiff_files = []\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.tif'):\n",
    "            tiff_files.append(path + '/' + f)\n",
    "\n",
    "    tiff_dict = {}\n",
    "\n",
    "    # dictionary of tiff files\n",
    "    for f in tiff_files:\n",
    "        k = f.split('/')[-1].split('.tif')[0]\n",
    "        tiff_dict[k] = f\n",
    "\n",
    "    # convert to np array\n",
    "    fire_id = Image.open(tiff_dict['fireid'])\n",
    "    fire_id = np.array(fire_id)\n",
    "    fire_id[fire_id == -9999] = 0\n",
    "\n",
    "    fireline = Image.open(tiff_dict['Global_fire_atlas_firelinecrop'])\n",
    "    fireline = np.array(fireline)\n",
    "    fireline[fireline == -9999] = 0\n",
    "\n",
    "    # get list of unique fire_ids\n",
    "    fire_ids = set()\n",
    "\n",
    "    for row in fire_id:\n",
    "        for val in row:\n",
    "            fire_ids.add(val)\n",
    "\n",
    "    # remove 0 from fire_ids set because it does not denote a fire\n",
    "    fire_ids.remove(0)\n",
    "\n",
    "    # get dict with key value pairs of fire_id and an empty dict\n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        id = str(id)\n",
    "        fire_data_dict[id] = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        indices = np.where(fire_id == id, 1, 0)\n",
    "        fire_data_dict[str(id)] = indices\n",
    "        \n",
    "    return fire_data_dict, fireline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_matrices(data_dict, fireline):\n",
    "    '''\n",
    "    Create matrices for each fire_id that show were the fire was on a given day during the year.\n",
    "    \n",
    "    Args:\n",
    "        - data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    Returns:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    '''\n",
    "    \n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for key, val in data_dict.items():\n",
    "        data = {}\n",
    "                \n",
    "        for y in range(1, 366):\n",
    "            mask = ((fireline == y) & (val == 1))\n",
    "            mask = mask.astype(int)\n",
    "        \n",
    "            if np.sum(mask) > 0:\n",
    "                data[str(y)] = mask\n",
    "        \n",
    "        fire_data_dict[key] = data\n",
    "        \n",
    "    return fire_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_day_pairs(fire_data_dict):\n",
    "    '''\n",
    "    Create a list of sets where the first value is where the fire was on a given day and the second value is where\n",
    "    the fire was on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    Returns:\n",
    "        - train_labels: a list of sets where the first value of the set is a one-hot encoded 2D array of fire \n",
    "        spread on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2:\n",
    "        [\n",
    "            (one-hot encoded 2D array of fire spread on that day_1, one-hot encoded 2D array of fire spread on day_2),\n",
    "            (one-hot encoded 2D array of fire spread on that day_2, one-hot encoded 2D array of fire spread on day_3),\n",
    "        ]\n",
    "    '''\n",
    "    \n",
    "    train_labels = []\n",
    "\n",
    "    for key, value in fire_data_dict.items():\n",
    "        burn_matrices = list(value.values())\n",
    "        day_of_year = list(value.keys())\n",
    "        \n",
    "        for index, day in enumerate(burn_matrices):\n",
    "\n",
    "            if index < len(burn_matrices) - 1:\n",
    "                day_1 = burn_matrices[index]\n",
    "                day_2_index = index + 1\n",
    "                day_2 = burn_matrices[day_2_index]\n",
    "                \n",
    "                doy = day_of_year[day_2_index]\n",
    "                \n",
    "                pair = (day_1, day_2)\n",
    "                train_labels.append((doy, pair))\n",
    "\n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_dict(directory, scaled_weather):\n",
    "    '''\n",
    "    Create a dictionary of weather data from a pickled file\n",
    "    Args:\n",
    "        - directory: path to weather pickle file\n",
    "        - scaled_weather: True/False to scale using max value\n",
    "    Returns:\n",
    "        - weather_data: dictionary of key (day of year) and value (dictionary of key (weather parameter) \n",
    "        and value (matrix of value for each pixel))\n",
    "    '''\n",
    "\n",
    "    path = os.path.abspath(directory)\n",
    "    \n",
    "    weather_file = ''\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.pickle'):\n",
    "            weather_file = path + '/' + f\n",
    "    \n",
    "    weather = pd.read_pickle(weather_file)\n",
    "    \n",
    "    weather_dict = {}\n",
    "    \n",
    "    for k, v in weather.items():\n",
    "        weather_dict[k] = {}\n",
    "        \n",
    "        for att, matrix in v.items():\n",
    "            mat = np.nan_to_num(matrix)\n",
    "            weather_dict[k][att] = mat\n",
    "     \n",
    "    weather_data = {}\n",
    "\n",
    "    for k, v in weather_dict.items():\n",
    "        doy = dt.strptime(k, \"%Y-%m-%d\").strftime(\"%j\")\n",
    "        weather_data[doy] = v\n",
    "    \n",
    "    # scale weather data\n",
    "    vals = list(weather_data.values())[0]\n",
    "    weather_atts = list(vals.keys())\n",
    "    max_values = dict.fromkeys(weather_atts, 0)\n",
    "    \n",
    "    if scaled_weather == True:\n",
    "        \n",
    "        for k, v in weather_dict.items():\n",
    "\n",
    "            for weather_att, matrix in v.items():\n",
    "                max_val = matrix.max()\n",
    "                if max_val > max_values[weather_att]:\n",
    "                    max_values[weather_att] = max_val\n",
    "    \n",
    "    return weather_data, max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(max_values, scaled_weather, day_of_year, x, y):\n",
    "    '''\n",
    "    Fetch weather data for the relevant day and pixel.\n",
    "    \n",
    "    Args:\n",
    "        - max_values: list of max_values for each weather features\n",
    "        - scaled_weather: whether the weather data should be scaled - true/false\n",
    "        - day_of_year: day of the year (1-365)\n",
    "        - x: x-coordinate of matrix\n",
    "        - y: y-coordinate of matrix\n",
    "    Returns:\n",
    "        - weather_list: an array of relevant weather data for that pixel\n",
    "    '''\n",
    "    weather_list = []\n",
    "    \n",
    "    day_weather = weather_data.get(day_of_year)\n",
    "\n",
    "    if day_weather is None:\n",
    "        return None\n",
    "    else:\n",
    "        for k, v in day_weather.items():\n",
    "            if scaled_weather == True:\n",
    "                max_val = max_values.get(k, 1)\n",
    "                \n",
    "                try:\n",
    "                    val = v[x,y]/max_val\n",
    "                    value = val/max_val\n",
    "                    \n",
    "                    if math.isnan(value):\n",
    "                        weather_list.append(0)\n",
    "                    else:\n",
    "                        weather_list.append(value)\n",
    "                except IndexError:\n",
    "                    return None\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    weather_list.append(v[x,y])\n",
    "                except IndexError:\n",
    "                    return None\n",
    "    \n",
    "    return weather_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Dataset for CNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset, matrix_dim, num_pixels, side):\n",
    "    '''\n",
    "    Supplement the list produced in `create_labeled_data` with data where there was no data\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "        - num_pixels: how many \"no-fire\" pixel-matrix pairs we want to return\n",
    "        - side: half the length of the dimension of the outpur matrix\n",
    "    Returns:\n",
    "        - no_fire: a list of sets, where the second value (0, 1) represents whether fire is present for a given pixel, and the\n",
    "        first value is a matrix centered on the second value for the previous day and represents where the fire was\n",
    "        on the previous day\n",
    "    '''\n",
    "        \n",
    "    no_fire = []\n",
    "\n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 0)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "        for (xi, yi) in vals:\n",
    "            xi_r = xi + side\n",
    "            xi_l = xi - side\n",
    "            yi_b = yi + side\n",
    "            yi_t = yi - side\n",
    "\n",
    "            m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "                                    \n",
    "            # control for edge cases where shape doesn't match up - not sure why this is happening\n",
    "            if m.shape == (matrix_dim, matrix_dim):\n",
    "                weather_data = fetch_weather_data(max_values, scaled_weather, doy, xi, yi)\n",
    "                if weather_data is not None:\n",
    "                    no_fire.append(((weather_data, m), 0))\n",
    "    \n",
    "    no_fire = random.sample(no_fire, num_pixels)\n",
    "    \n",
    "    return no_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_data(dataset, matrix_dim):\n",
    "    '''\n",
    "    Create a list of sets where the first value is a matrix of pixels on a given day and the second value denotes\n",
    "    whether there was fire in the center pixel on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "    Returns:\n",
    "        - data: a list of sets, where the second value (0, 1) represents whether fire is present for a given pixel, and the\n",
    "        first value is a matrix centered on the second value for the previous day and represents where the fire was\n",
    "        on the previous day\n",
    "    '''\n",
    "\n",
    "    side = int(matrix_dim/2)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 1)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "        for (xi, yi) in vals:\n",
    "            xi_r = xi + side\n",
    "            xi_l = xi - side\n",
    "            yi_b = yi + side\n",
    "            yi_t = yi - side\n",
    "\n",
    "            m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "                        \n",
    "            weather_data = fetch_weather_data(max_values, scaled_weather, doy, xi, yi)\n",
    "            \n",
    "            if weather_data is not None:\n",
    "                data.append(((weather_data, m), 1))\n",
    "    \n",
    "    data_len = len(data)\n",
    "    num_pixels = min(int(data_len*labeled_multiplier), data_len)\n",
    "    \n",
    "    # balance this dataset with values where there is no fire\n",
    "    no_fire = balance_dataset(dataset, matrix_dim, num_pixels, side)\n",
    "    \n",
    "    # combine and shuffle\n",
    "    data += no_fire    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset_for_cnn(data):\n",
    "    '''\n",
    "    Takes a list of ((list, matrix), integer) pairs and returns fire data, weather data, and output labels \n",
    "    split into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        - data: a list of (matrix, integer) pairs\n",
    "    Returns:\n",
    "        - fire: array of input data in matrix_dim X matrix_dim shape\n",
    "        - weather: list of scaled weather weights\n",
    "        - Y: array of output labels (0 or 1)\n",
    "    '''\n",
    "    \n",
    "    fire = []\n",
    "    weather = []\n",
    "    Y = []\n",
    "\n",
    "    for ((w, f), y) in data:\n",
    "        f = np.asarray(f)\n",
    "        fire.append(f)\n",
    "        \n",
    "        w = np.asarray(w)\n",
    "        weather.append(w)\n",
    "        \n",
    "        Y.append(y)\n",
    "\n",
    "    fire = np.asarray(fire)\n",
    "    weather = np.asarray(weather)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    obs = len(fire)\n",
    "    \n",
    "    fire = fire.reshape(obs, matrix_dim, matrix_dim, 1)\n",
    "\n",
    "    return fire, weather, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data, max_values = create_weather_dict(weather_directory, scaled_weather)\n",
    "fire_data_dict, fireline = data_processing(tif_directory)\n",
    "fire_data_dict = create_one_hot_matrices(fire_data_dict, fireline)\n",
    "small_dataset = create_day_pairs(fire_data_dict)\n",
    "data = create_labeled_data(small_dataset, matrix_dim)\n",
    "fire, weather, Y = prep_dataset_for_cnn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import AveragePooling2D, Conv1D, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1 score manually\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_pos / (possible_pos + K.epsilon())\n",
    "\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_pos / (predicted_pos + K.epsilon())\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    score = 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Fire Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_1: fire image data with Sequential API\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model_1.add(AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid'))\n",
    "model_1.add(Conv2D(64, kernel_size=(3, 3), activation='sigmoid'))\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# Final dense layer \n",
    "model_1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_1.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy', f1_score, tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5409 samples, validate on 1353 samples\n",
      "Epoch 1/30\n",
      "5409/5409 [==============================] - 2s 331us/step - loss: 0.6979 - accuracy: 0.5062 - f1_score: 0.4614 - auc_49: 0.5047 - val_loss: 0.6934 - val_accuracy: 0.4945 - val_f1_score: 0.0000e+00 - val_auc_49: 0.5176\n",
      "Epoch 2/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.6271 - accuracy: 0.6293 - f1_score: 0.6010 - auc_49: 0.5421 - val_loss: 0.4840 - val_accuracy: 0.5055 - val_f1_score: 0.6691 - val_auc_49: 0.6048\n",
      "Epoch 3/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.2813 - accuracy: 0.9018 - f1_score: 0.8895 - auc_49: 0.6925 - val_loss: 0.4235 - val_accuracy: 0.5055 - val_f1_score: 0.6691 - val_auc_49: 0.7560\n",
      "Epoch 4/30\n",
      "5409/5409 [==============================] - 2s 286us/step - loss: 0.1947 - accuracy: 0.9327 - f1_score: 0.9212 - auc_49: 0.7938 - val_loss: 0.1720 - val_accuracy: 0.9808 - val_f1_score: 0.9797 - val_auc_49: 0.8350\n",
      "Epoch 5/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1579 - accuracy: 0.9523 - f1_score: 0.9482 - auc_49: 0.8621 - val_loss: 0.1201 - val_accuracy: 0.9660 - val_f1_score: 0.9643 - val_auc_49: 0.8835\n",
      "Epoch 6/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1297 - accuracy: 0.9604 - f1_score: 0.9521 - auc_49: 0.8990 - val_loss: 0.1018 - val_accuracy: 0.9593 - val_f1_score: 0.9561 - val_auc_49: 0.9120\n",
      "Epoch 7/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.1164 - accuracy: 0.9651 - f1_score: 0.9622 - auc_49: 0.9218 - val_loss: 0.0942 - val_accuracy: 0.9845 - val_f1_score: 0.9838 - val_auc_49: 0.9304\n",
      "Epoch 8/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.1087 - accuracy: 0.9700 - f1_score: 0.9686 - auc_49: 0.9372 - val_loss: 0.0766 - val_accuracy: 0.9837 - val_f1_score: 0.9831 - val_auc_49: 0.9432\n",
      "Epoch 9/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.1057 - accuracy: 0.9725 - f1_score: 0.9648 - auc_49: 0.9480 - val_loss: 0.0761 - val_accuracy: 0.9727 - val_f1_score: 0.9712 - val_auc_49: 0.9522\n",
      "Epoch 10/30\n",
      "5409/5409 [==============================] - 1s 272us/step - loss: 0.0934 - accuracy: 0.9750 - f1_score: 0.9737 - auc_49: 0.9559 - val_loss: 0.0696 - val_accuracy: 0.9778 - val_f1_score: 0.9768 - val_auc_49: 0.9591\n",
      "Epoch 11/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.0944 - accuracy: 0.9763 - f1_score: 0.9744 - auc_49: 0.9617 - val_loss: 0.0692 - val_accuracy: 0.9778 - val_f1_score: 0.9770 - val_auc_49: 0.9642\n",
      "Epoch 12/30\n",
      "5409/5409 [==============================] - 2s 288us/step - loss: 0.0846 - accuracy: 0.9802 - f1_score: 0.9797 - auc_49: 0.9664 - val_loss: 0.0555 - val_accuracy: 0.9860 - val_f1_score: 0.9855 - val_auc_49: 0.9685\n",
      "Epoch 13/30\n",
      "5409/5409 [==============================] - 2s 290us/step - loss: 0.0836 - accuracy: 0.9791 - f1_score: 0.9728 - auc_49: 0.9702 - val_loss: 0.0574 - val_accuracy: 0.9926 - val_f1_score: 0.9925 - val_auc_49: 0.9719\n",
      "Epoch 14/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0768 - accuracy: 0.9845 - f1_score: 0.9839 - auc_49: 0.9733 - val_loss: 0.0848 - val_accuracy: 0.9956 - val_f1_score: 0.9954 - val_auc_49: 0.9747\n",
      "Epoch 15/30\n",
      "5409/5409 [==============================] - 2s 284us/step - loss: 0.0777 - accuracy: 0.9821 - f1_score: 0.9758 - auc_49: 0.9757 - val_loss: 0.0517 - val_accuracy: 0.9852 - val_f1_score: 0.9848 - val_auc_49: 0.9769\n",
      "Epoch 16/30\n",
      "5409/5409 [==============================] - 2s 299us/step - loss: 0.0705 - accuracy: 0.9854 - f1_score: 0.9841 - auc_49: 0.9779 - val_loss: 0.0453 - val_accuracy: 0.9911 - val_f1_score: 0.9911 - val_auc_49: 0.9789\n",
      "Epoch 17/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0728 - accuracy: 0.9828 - f1_score: 0.9822 - auc_49: 0.9798 - val_loss: 0.0483 - val_accuracy: 0.9941 - val_f1_score: 0.9941 - val_auc_49: 0.9806\n",
      "Epoch 18/30\n",
      "5409/5409 [==============================] - 2s 288us/step - loss: 0.0704 - accuracy: 0.9852 - f1_score: 0.9844 - auc_49: 0.9813 - val_loss: 0.0529 - val_accuracy: 0.9860 - val_f1_score: 0.9859 - val_auc_49: 0.9820\n",
      "Epoch 19/30\n",
      "5409/5409 [==============================] - 2s 285us/step - loss: 0.0670 - accuracy: 0.9858 - f1_score: 0.9839 - auc_49: 0.9826 - val_loss: 0.0764 - val_accuracy: 0.9956 - val_f1_score: 0.9954 - val_auc_49: 0.9831\n",
      "Epoch 20/30\n",
      "5409/5409 [==============================] - 2s 311us/step - loss: 0.0657 - accuracy: 0.9869 - f1_score: 0.9865 - auc_49: 0.9837 - val_loss: 0.0415 - val_accuracy: 0.9889 - val_f1_score: 0.9890 - val_auc_49: 0.9842\n",
      "Epoch 21/30\n",
      "5409/5409 [==============================] - 2s 279us/step - loss: 0.0643 - accuracy: 0.9852 - f1_score: 0.9787 - auc_49: 0.9847 - val_loss: 0.0399 - val_accuracy: 0.9941 - val_f1_score: 0.9941 - val_auc_49: 0.9851\n",
      "Epoch 22/30\n",
      "5409/5409 [==============================] - 2s 283us/step - loss: 0.0657 - accuracy: 0.9869 - f1_score: 0.9866 - auc_49: 0.9856 - val_loss: 0.0387 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9860\n",
      "Epoch 23/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0564 - accuracy: 0.9878 - f1_score: 0.9817 - auc_49: 0.9864 - val_loss: 0.0374 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9867\n",
      "Epoch 24/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.0571 - accuracy: 0.9880 - f1_score: 0.9878 - auc_49: 0.9871 - val_loss: 0.0414 - val_accuracy: 0.9897 - val_f1_score: 0.9896 - val_auc_49: 0.9874\n",
      "Epoch 25/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0573 - accuracy: 0.9869 - f1_score: 0.9806 - auc_49: 0.9878 - val_loss: 0.0401 - val_accuracy: 0.9897 - val_f1_score: 0.9897 - val_auc_49: 0.9880\n",
      "Epoch 26/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0523 - accuracy: 0.9893 - f1_score: 0.9890 - auc_49: 0.9883 - val_loss: 0.0371 - val_accuracy: 0.9904 - val_f1_score: 0.9905 - val_auc_49: 0.9886\n",
      "Epoch 27/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.0530 - accuracy: 0.9889 - f1_score: 0.9881 - auc_49: 0.9889 - val_loss: 0.0371 - val_accuracy: 0.9911 - val_f1_score: 0.9912 - val_auc_49: 0.9891\n",
      "Epoch 28/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0516 - accuracy: 0.9882 - f1_score: 0.9875 - auc_49: 0.9893 - val_loss: 0.0340 - val_accuracy: 0.9948 - val_f1_score: 0.9949 - val_auc_49: 0.9896\n",
      "Epoch 29/30\n",
      "5409/5409 [==============================] - 2s 285us/step - loss: 0.0505 - accuracy: 0.9887 - f1_score: 0.9822 - auc_49: 0.9898 - val_loss: 0.0389 - val_accuracy: 0.9889 - val_f1_score: 0.9892 - val_auc_49: 0.9900\n",
      "Epoch 30/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.0508 - accuracy: 0.9889 - f1_score: 0.9885 - auc_49: 0.9902 - val_loss: 0.0348 - val_accuracy: 0.9919 - val_f1_score: 0.9921 - val_auc_49: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f89b7996b50>"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_1.fit(\n",
    "    x = fire, \n",
    "    y = Y,\n",
    "    validation_split = test_size, \n",
    "    epochs=epoc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00762875],\n",
       "       [1.        ],\n",
       "       [0.00762875],\n",
       "       [0.99789375],\n",
       "       [0.00762875],\n",
       "       [0.00762875],\n",
       "       [0.00762875],\n",
       "       [0.9999999 ],\n",
       "       [0.00762875],\n",
       "       [0.99999976]], dtype=float32)"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.predict(fire[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Fire Image Data and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_2: image data and weather data with functional API\n",
    "\n",
    "# Define image inputs shape\n",
    "image_shape = fire[0].shape\n",
    "image_inputs = Input(shape = image_shape)\n",
    "\n",
    "# Define weather inputs shape\n",
    "weather_shape = weather[0].shape\n",
    "weather_inputs = Input(shape = weather_shape)\n",
    "\n",
    "# Add layers for fire image interpretation\n",
    "fire_1 = AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid')(image_inputs)\n",
    "fire_2 = Conv2D(64, kernel_size=(3, 3), activation='sigmoid')(fire_1)\n",
    "fire_3 = Conv2D(32, kernel_size=(3, 3), activation='sigmoid')(fire_2)\n",
    "fire_4 = MaxPooling2D(pool_size=(2,2), strides=None, padding='valid')(fire_3)\n",
    "fire_5 = Dropout(0.2)(fire_4)\n",
    "fire_6 = Flatten()(fire_5)\n",
    "fire_7 = Dense(8, activation='sigmoid')(fire_6)\n",
    "\n",
    "# Combine the layers\n",
    "concat = concatenate([fire_7, weather_inputs])\n",
    "\n",
    "# Final dense layer \n",
    "predictions = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "# Define the model\n",
    "model_2 = Model(inputs=[image_inputs, weather_inputs], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy', f1_score, tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5409 samples, validate on 1353 samples\n",
      "Epoch 1/30\n",
      "5409/5409 [==============================] - 2s 327us/step - loss: 19.5171 - accuracy: 0.5304 - f1_score: 0.6709 - auc_53: 0.5331 - val_loss: 6.9409 - val_accuracy: 0.5469 - val_f1_score: 0.6844 - val_auc_53: 0.5376\n",
      "Epoch 2/30\n",
      "5409/5409 [==============================] - 1s 269us/step - loss: 1.8920 - accuracy: 0.5178 - f1_score: 0.5075 - auc_53: 0.5321 - val_loss: 1.0869 - val_accuracy: 0.5174 - val_f1_score: 0.4598 - val_auc_53: 0.5312\n",
      "Epoch 3/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 1.0102 - accuracy: 0.5215 - f1_score: 0.4640 - auc_53: 0.5329 - val_loss: 0.9475 - val_accuracy: 0.5292 - val_f1_score: 0.4765 - val_auc_53: 0.5368\n",
      "Epoch 4/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 0.8912 - accuracy: 0.5275 - f1_score: 0.4691 - auc_53: 0.5410 - val_loss: 0.8421 - val_accuracy: 0.5536 - val_f1_score: 0.5348 - val_auc_53: 0.5461\n",
      "Epoch 5/30\n",
      "5409/5409 [==============================] - 1s 269us/step - loss: 0.8139 - accuracy: 0.5421 - f1_score: 0.5013 - auc_53: 0.5509 - val_loss: 0.7796 - val_accuracy: 0.5898 - val_f1_score: 0.5925 - val_auc_53: 0.5563\n",
      "Epoch 6/30\n",
      "5409/5409 [==============================] - 1s 270us/step - loss: 0.7712 - accuracy: 0.5713 - f1_score: 0.5387 - auc_53: 0.5614 - val_loss: 0.7397 - val_accuracy: 0.5898 - val_f1_score: 0.5763 - val_auc_53: 0.5664\n",
      "Epoch 7/30\n",
      "5409/5409 [==============================] - 1s 271us/step - loss: 0.7462 - accuracy: 0.5837 - f1_score: 0.5610 - auc_53: 0.5712 - val_loss: 0.7175 - val_accuracy: 0.5935 - val_f1_score: 0.5688 - val_auc_53: 0.5757\n",
      "Epoch 8/30\n",
      "5409/5409 [==============================] - 1s 269us/step - loss: 0.7293 - accuracy: 0.6064 - f1_score: 0.5960 - auc_53: 0.5799 - val_loss: 0.6987 - val_accuracy: 0.6120 - val_f1_score: 0.5904 - val_auc_53: 0.5842\n",
      "Epoch 9/30\n",
      "5409/5409 [==============================] - 1s 271us/step - loss: 0.7142 - accuracy: 0.6186 - f1_score: 0.6096 - auc_53: 0.5878 - val_loss: 0.6815 - val_accuracy: 0.6652 - val_f1_score: 0.6792 - val_auc_53: 0.5918\n",
      "Epoch 10/30\n",
      "5409/5409 [==============================] - 1s 271us/step - loss: 0.7015 - accuracy: 0.6334 - f1_score: 0.6196 - auc_53: 0.5953 - val_loss: 0.6685 - val_accuracy: 0.6681 - val_f1_score: 0.6620 - val_auc_53: 0.5989\n",
      "Epoch 11/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.6884 - accuracy: 0.6449 - f1_score: 0.6386 - auc_53: 0.6022 - val_loss: 0.6590 - val_accuracy: 0.6659 - val_f1_score: 0.6471 - val_auc_53: 0.6054\n",
      "Epoch 12/30\n",
      "5409/5409 [==============================] - 1s 273us/step - loss: 0.6778 - accuracy: 0.6467 - f1_score: 0.6361 - auc_53: 0.6083 - val_loss: 0.6422 - val_accuracy: 0.6814 - val_f1_score: 0.6905 - val_auc_53: 0.6115\n",
      "Epoch 13/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.6627 - accuracy: 0.6591 - f1_score: 0.6539 - auc_53: 0.6142 - val_loss: 0.6312 - val_accuracy: 0.6925 - val_f1_score: 0.7077 - val_auc_53: 0.6172\n",
      "Epoch 14/30\n",
      "5409/5409 [==============================] - 2s 282us/step - loss: 0.6533 - accuracy: 0.6678 - f1_score: 0.6646 - auc_53: 0.6199 - val_loss: 0.6278 - val_accuracy: 0.6814 - val_f1_score: 0.6569 - val_auc_53: 0.6227\n",
      "Epoch 15/30\n",
      "5409/5409 [==============================] - 2s 301us/step - loss: 0.6444 - accuracy: 0.6778 - f1_score: 0.6750 - auc_53: 0.6253 - val_loss: 0.6248 - val_accuracy: 0.6763 - val_f1_score: 0.6408 - val_auc_53: 0.6278\n",
      "Epoch 16/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.6369 - accuracy: 0.6763 - f1_score: 0.6685 - auc_53: 0.6305 - val_loss: 0.6225 - val_accuracy: 0.6718 - val_f1_score: 0.6254 - val_auc_53: 0.6328\n",
      "Epoch 17/30\n",
      "5409/5409 [==============================] - 2s 280us/step - loss: 0.6313 - accuracy: 0.6774 - f1_score: 0.6774 - auc_53: 0.6349 - val_loss: 0.5970 - val_accuracy: 0.7162 - val_f1_score: 0.7248 - val_auc_53: 0.6374\n",
      "Epoch 18/30\n",
      "5409/5409 [==============================] - 1s 277us/step - loss: 0.6250 - accuracy: 0.6879 - f1_score: 0.6850 - auc_53: 0.6396 - val_loss: 0.5928 - val_accuracy: 0.7132 - val_f1_score: 0.7050 - val_auc_53: 0.6417\n",
      "Epoch 19/30\n",
      "5409/5409 [==============================] - 2s 289us/step - loss: 0.6172 - accuracy: 0.6914 - f1_score: 0.6916 - auc_53: 0.6438 - val_loss: 0.5861 - val_accuracy: 0.7162 - val_f1_score: 0.7112 - val_auc_53: 0.6459\n",
      "Epoch 20/30\n",
      "5409/5409 [==============================] - 2s 310us/step - loss: 0.6138 - accuracy: 0.6876 - f1_score: 0.6831 - auc_53: 0.6478 - val_loss: 0.5989 - val_accuracy: 0.6844 - val_f1_score: 0.6381 - val_auc_53: 0.6498\n",
      "Epoch 21/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.6104 - accuracy: 0.6940 - f1_score: 0.6877 - auc_53: 0.6516 - val_loss: 0.5834 - val_accuracy: 0.7184 - val_f1_score: 0.7458 - val_auc_53: 0.6534\n",
      "Epoch 22/30\n",
      "5409/5409 [==============================] - 1s 275us/step - loss: 0.6046 - accuracy: 0.6972 - f1_score: 0.6933 - auc_53: 0.6550 - val_loss: 0.5717 - val_accuracy: 0.7288 - val_f1_score: 0.7379 - val_auc_53: 0.6569\n",
      "Epoch 23/30\n",
      "5409/5409 [==============================] - 1s 274us/step - loss: 0.6035 - accuracy: 0.6883 - f1_score: 0.6769 - auc_53: 0.6584 - val_loss: 0.5697 - val_accuracy: 0.7184 - val_f1_score: 0.7106 - val_auc_53: 0.6602\n",
      "Epoch 24/30\n",
      "5409/5409 [==============================] - 2s 284us/step - loss: 0.5964 - accuracy: 0.7023 - f1_score: 0.7026 - auc_53: 0.6618 - val_loss: 0.5671 - val_accuracy: 0.7332 - val_f1_score: 0.7497 - val_auc_53: 0.6634\n",
      "Epoch 25/30\n",
      "5409/5409 [==============================] - 2s 278us/step - loss: 0.5927 - accuracy: 0.7031 - f1_score: 0.6996 - auc_53: 0.6649 - val_loss: 0.5636 - val_accuracy: 0.7221 - val_f1_score: 0.7147 - val_auc_53: 0.6664\n",
      "Epoch 26/30\n",
      "5409/5409 [==============================] - 2s 287us/step - loss: 0.5918 - accuracy: 0.7075 - f1_score: 0.7032 - auc_53: 0.6679 - val_loss: 0.5602 - val_accuracy: 0.7258 - val_f1_score: 0.7289 - val_auc_53: 0.6694\n",
      "Epoch 27/30\n",
      "5409/5409 [==============================] - 2s 285us/step - loss: 0.5892 - accuracy: 0.7031 - f1_score: 0.7064 - auc_53: 0.6708 - val_loss: 0.5683 - val_accuracy: 0.7184 - val_f1_score: 0.7503 - val_auc_53: 0.6721\n",
      "Epoch 28/30\n",
      "5409/5409 [==============================] - 2s 286us/step - loss: 0.5903 - accuracy: 0.7016 - f1_score: 0.6990 - auc_53: 0.6732 - val_loss: 0.5602 - val_accuracy: 0.7302 - val_f1_score: 0.7538 - val_auc_53: 0.6745\n",
      "Epoch 29/30\n",
      "5409/5409 [==============================] - 2s 294us/step - loss: 0.5855 - accuracy: 0.7059 - f1_score: 0.7112 - auc_53: 0.6758 - val_loss: 0.5563 - val_accuracy: 0.7413 - val_f1_score: 0.7596 - val_auc_53: 0.6770\n",
      "Epoch 30/30\n",
      "5409/5409 [==============================] - 1s 276us/step - loss: 0.5837 - accuracy: 0.7055 - f1_score: 0.7050 - auc_53: 0.6782 - val_loss: 0.5540 - val_accuracy: 0.7243 - val_f1_score: 0.7230 - val_auc_53: 0.6793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f89fe07ed90>"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_2.fit(\n",
    "    x = [fire, weather], \n",
    "    y = Y,\n",
    "    validation_split = test_size, \n",
    "    epochs=epoc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33487126],\n",
       "       [0.8180104 ],\n",
       "       [0.13982786],\n",
       "       [0.63246477],\n",
       "       [0.4823586 ],\n",
       "       [0.4553281 ],\n",
       "       [0.5802383 ],\n",
       "       [0.73576325],\n",
       "       [0.3785281 ],\n",
       "       [0.41456392]], dtype=float32)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.predict([fire[:10], weather[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
