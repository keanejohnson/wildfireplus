{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib 3.2.0\n",
      "seaborn 0.10.0\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(\"matplotlib\",matplotlib.__version__)\n",
    "print(\"seaborn\",sns.__version__)\n",
    "print(\"numpy\",np.__version__)\n",
    "print(\"pandas\",pd.__version__)\n",
    "print(\"python\",sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Requests \n",
    "\n",
    "From GEO EDA sheet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## DarkSky Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "1) cheapest seems to be DarkSky. 1000 per day for free 2) $0.0001 per request after that 3) Request: \" https://api.darksky.net/forecast/[key]/40.036944,-121.005833,2005-02-02T17:30:00\" (my api key removed)\n",
    "\n",
    "Output is like this: {\"latitude\":40.036944,\"longitude\":-121.005833,\"timezone\":\"America/Los_Angeles\",\"currently\":{\"time\":1107394200,\"precipIntensity\":0,\"precipProbability\":0,\"temperature\":50.24,\"apparentTemperature\":50.41,\"dewPoint\":13.85,\"humidity\":0.23,\"windSpeed\":2.62,\"windGust\":13.17,\"windBearing\":24,\"uvIndex\":0},\"hourly\":{\"data\":\n",
    "...\n",
    "{\"time\":1107414000,\"precipIntensity\":0,\"precipProbability\":0,\"temperature\":41.2,\"apparentTemperature\":41.2,\"dewPoint\":12.53,\"humidity\":0.31,\"windSpeed\":1.56,\"windGust\":12.79,\"windBearing\":100,\"uvIndex\":0}]},\"daily\":{\"data\":[{\"time\":1107331200,\"sunriseTime\":1107357120,\"sunsetTime\":1107393960,\"moonPhase\":0.78,\"precipIntensity\":0,\"precipIntensityMax\":0,\"precipProbability\":0,\"temperatureHigh\":57.95,\"temperatureHighTime\":1107384600,\"temperatureLow\":38.42,\"temperatureLowTime\":1107439200,\"apparentTemperatureHigh\":57.45,\"apparentTemperatureHighTime\":1107384600,\"apparentTemperatureLow\":38.91,\"apparentTemperatureLowTime\":1107439200,\"dewPoint\":14.85,\"humidity\":0.31,\"windSpeed\":3.33,\"windGust\":14.83,\"windGustTime\":1107390840,\"windBearing\":76,\"uvIndex\":0,\"uvIndexTime\":1107331200,\"temperatureMin\":35.03,\"temperatureMinTime\":1107359100,\"temperatureMax\":57.95,\"temperatureMaxTime\":1107384600,\"apparentTemperatureMin\":34.3,\"apparentTemperatureMinTime\":1107338640,\"apparentTemperatureMax\":57.45,\"apparentTemperatureMaxTime\":1107384600}]},\"flags\":{\"sources\":[\"cmc\",\"gfs\",\"hrrr\",\"icon\",\"isd\",\"madis\",\"nam\",\"sref\"],\"nearest-station\":3.312,\"units\":\"us\"},\"offset\":-8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Dark Sky locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# From Geo Sheet - Toy Data Bounding Box\n",
    "# keep in mind each pixel is roughly 500 m x 500 m\n",
    "bb_long = [-122, -119.912,-119.912,-122,-122 ]\n",
    "bb_lat = [36.8, 36.8, 35.06, 35.06,36.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Requests, to see how how 'hyperlocal' the darksky data really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create Pairs\n",
    "min_long = min(bb_long)\n",
    "max_long = max(bb_long)\n",
    "min_lat = min(bb_lat)\n",
    "max_lat = max(bb_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "spaces = 10\n",
    "longs = np.linspace(min_long,max_long,spaces)\n",
    "lats = np.linspace(min_lat,max_lat,spaces)\n",
    "print(longs, lats)\n",
    "distance = (max_lat-min_lat)*110/spaces\n",
    "print(f\"distance between forecast pairings ~ {distance:.3} km, or {(distance/.5):.3} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Pairs\n",
    "pairs = [(lat,long) for lat in lats for long in longs]\n",
    "print(len(pairs))\n",
    "print(pairs[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "key = '5ffac5f056d341c6296cba58fa96e9ba'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Forecast Data API supports HTTP compression. We heartily recommend using it, as it will make responses much smaller over the wire. To enable it, simply add an `Accept-Encoding: gzip` header to your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "### Try one request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# items to exclude from call\n",
    "blocks = '[currently,minutely,hourly,alerts]'\n",
    "# Units for call\n",
    "units = 'ca'\n",
    "# Time\n",
    "time = '2005-02-02T12:00:00' # for noon, but hour doesn't matter as we're grabbing daily data only.\n",
    "# Dates (relevant to our fires): only use one for testing\n",
    "dates = ['2016-12-16']\n",
    "\n",
    "# create time string:\n",
    "date = dates[0]\n",
    "time = date+'T12:00:00'\n",
    "lat = str(pairs[25][0])\n",
    "long = str(pairs[25][1])\n",
    "\n",
    "query = ('https://api.darksky.net/forecast/'+key+'/'+ \n",
    "        lat+','+long+','+time+'?exclude=' \n",
    "        +blocks+'&units='+units)\n",
    "headers = {'Accept-Encoding':'gzip'}\n",
    "print(query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d = requests.get('https://api.darksky.net/forecast/5ffac5f056d341c6296cba58fa96e9ba/36.8,-122.0,2016-07-21T12:00:00?exclude=[currently,minutely,hourly,alerts]&units=ca',headers=headers)\n",
    "d.json()['daily']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r=requests.get(query,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather = r.json()\n",
    "weather['daily']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parsed important elements:\n",
    "dailydata = weather['daily']['data'][0]\n",
    "rainint = dailydata['precipIntensityMax']\n",
    "raintot = dailydata['precipAccumulation']=0.0\n",
    "hitemp = dailydata['temperatureHigh']\n",
    "lotemp = dailydata['temperatureLow']\n",
    "humidity = dailydata['humidity']\n",
    "windspd = dailydata['windSpeed']\n",
    "winddir = dailydata['windBearing']\n",
    "clouds = dailydata['cloudCover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "point = [date,lat,long,rainint,raintot,hitemp,lotemp,humidity,windspd,winddir,clouds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(type(point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pairs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Multiple Requests to determine degree of hyperlocality required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "### Https requests functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# NEWER VERSION BELOW\n",
    "# def getweather(coordinates,date,key):\n",
    "#     \"\"\" Function to take the set of coordinates and get the daily\n",
    "#     weather for a given date. rain, hi and low temps, humidity\n",
    "#     wind speed and direction and cloud cover only.\n",
    "    \n",
    "#     Input: Date, API Key and list of coordinate pairs (lat,long)\n",
    "#     Output: list of jsons\n",
    "#     \"\"\"\n",
    "#     # items to exclude from call\n",
    "#     blocks = '[currently,minutely,hourly,alerts]'\n",
    "\n",
    "#     # Units for call\n",
    "#     units = 'ca'\n",
    "    \n",
    "#     # Compression for call\n",
    "#     headers = {'Accept-Encoding':'gzip'}\n",
    "    \n",
    "#     time = date+'T12:00:00'\n",
    "    \n",
    "#     data_out = []\n",
    "#     for pair in coordinates:\n",
    "#         lat = str(pair[0])\n",
    "#         long = str(pair[1])\n",
    "#         # set the query string for darksky\n",
    "#         query = ('https://api.darksky.net/forecast/'+key+'/'+ \n",
    "#             lat+','+long+','+time+'?exclude=' \n",
    "#             +blocks+'&units='+units)\n",
    "#         # Make the call to Dark Sky to get all the data for that date and location\n",
    "#         r=requests.get(query,headers=headers)\n",
    "        \n",
    "#         # get the weather data from the request return\n",
    "#         weather=r.json()\n",
    "        \n",
    "#         # write the jsons as items in the output data list\n",
    "#         data_out.append(weather)\n",
    "    \n",
    "#     return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dates (relevant to our fires): only use one for testing\n",
    "dates = ['2016-07-21']\n",
    "# create time string:\n",
    "date = dates[0]\n",
    "\n",
    "data_for_date = getweather(pairs,date,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pickle results out so that i don't have to make another api call later\n",
    "with open('../data/GlobalFire2016/weathertest.pickle','wb') as f:\n",
    "    pickle.dump(data_for_date,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_point(info,rain=False):\n",
    "    \"\"\" to check if there is a value for that weather element and provide \n",
    "    either None or the value back\n",
    "    \"\"\"\n",
    "    if info:\n",
    "        return info\n",
    "    elif (not info and rain):\n",
    "        return 0.0        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_df(data_out):\n",
    "    \"\"\" Function to take the list of jsons from a date\n",
    "    of daily weather requests and create a pandas dataframe\n",
    "    \n",
    "    Input: list of jsons containing the weather data\n",
    "    Output: pandas dataframe\n",
    "    \"\"\"\n",
    "    data_convert=[]\n",
    "    # Titles for the data\n",
    "    coltitles = ['date','latitude','longitude','rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']\n",
    "    data_convert.append(coltitles)\n",
    "    # Loope through all the geopoints to get the data\n",
    "    for point in data_out:\n",
    "        lat = point['latitude']\n",
    "        long = point['longitude']\n",
    "        daily = point.get('daily')\n",
    "        if daily:\n",
    "            data = daily['data'][0]\n",
    "            time = datetime.fromtimestamp(data['time']).strftime('%Y-%m-%d')\n",
    "            rainint = data.get('precipIntensityMax')\n",
    "            raintot = data.get('precipAccumulation')\n",
    "            hitemp = data.get('temperatureHigh')\n",
    "            lotemp = data.get('temperatureLow')\n",
    "            humidity = data.get('humidity')\n",
    "            windspd = data.get('windSpeed')\n",
    "            winddir = data.get('windBearing')\n",
    "            clouds = data.get('cloudCover')\n",
    "            point = [date,lat,long,check_point(rainint,1),check_point(raintot,1), \\\n",
    "                     check_point(hitemp,0),check_point(lotemp,0), \\\n",
    "                     check_point(humidity,0),check_point(windspd,0), \\\n",
    "                     check_point(winddir,0), check_point(clouds,0)] \n",
    "            data_convert.append(point)\n",
    "        else:\n",
    "            point = None\n",
    "            \n",
    "    df = pd.DataFrame(data_convert[1:], columns = data_convert[0])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('../data/GlobalFire2016/weathertest.pickle','rb') as f:\n",
    "    data_for_date = pickle.load(f)\n",
    "    \n",
    "    # Dates (relevant to our fires): only use one for testing\n",
    "dates = ['2016-07-21']\n",
    "# create time string:\n",
    "date = dates[0]\n",
    "df2 = date_df(data_for_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If matrix is missing some rows then add them back in with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df3 = copy.deepcopy(df2)\n",
    "df3['xx'] = list(zip(df3['latitude'],df3['longitude']))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "missing = pd.DataFrame({'xx':list(set(df3['xx']) ^ set(pairs))})\n",
    "missing[['latitude','longitude']]=pd.DataFrame(missing['xx'].tolist(),index=missing.index)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df4 = df3.append(missing, ignore_index=True, sort=True)\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Look at uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats = df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cols = stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "uniques = []\n",
    "for col in cols:\n",
    "    uniq = len(df2[col].value_counts())\n",
    "    uniques.append(uniq)\n",
    "uniques = pd.DataFrame([uniques],columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "uniques.rename(index={0:'Uniques'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "newstats = stats.append(uniques)\n",
    "newstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Turn data into 2D numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The above says that the data is fairly hyperlocal. Try doing heat maps?\n",
    "To do heatmaps, have to turn this into a numpy array which i have to do anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create the numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Sample for 1 array (High Temp) For Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create numpy array for High Temperature\n",
    "# create a list of all the values by increasing longitude for each latitude, so we can vstack into array\n",
    "\n",
    "# # Subset the dataframe\n",
    "# sub = df2[['latitude','longitude','High T']]\n",
    "\n",
    "# # All values of longitude, to ensure square matrix\n",
    "# long_df = pd.DataFrame(longs,columns=['longitude'])\n",
    "\n",
    "# hiT = []\n",
    "\n",
    "# # reverse the latitudes so we start with largest first\n",
    "# lats2 = list(copy.deepcopy(lats))\n",
    "# lats2.reverse()\n",
    "\n",
    "# # loop through each latitude\n",
    "# for lat in lats2:\n",
    "#     # get the row values for each latitude, sort by longitude\n",
    "#     row = sub[sub['latitude']==lat].sort_values(by=['longitude'])\n",
    "    \n",
    "#     # add in missing longitude if necessary\n",
    "#     row2 = long_df.merge(row,how='left')\n",
    "    \n",
    "#     # Fill any missing values of latitude if needed\n",
    "#     row2['latitude'].fillna(lat,inplace=True)\n",
    "    \n",
    "#     # pull out just the High T\n",
    "#     row3 = row2['High T']\n",
    "    \n",
    "#     # reshape array\n",
    "#     row3 = np.array(row3)[np.newaxis] \n",
    "    \n",
    "#     hiT.append(row3)\n",
    "\n",
    "# x = np.vstack(hiT)\n",
    "# For Reference\n",
    "# hhh = HighTempArray\n",
    "# hhh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Looping to generate all weather for given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather = ['rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weath_sort = df4.to_records(index=False)\n",
    "weath_sort.sort(order=('latitude','longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather_date = {}\n",
    "for var in weather:\n",
    "    x = weath_sort[var].reshape(10,10)\n",
    "    xx = np.flip(x,0)\n",
    "    weather_date.update({var:xx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# REFERENCE PLOT\n",
    "# ax = sns.heatmap(HighTempArray)  # Plotting HighTempArray but replaced -9999 with nan for plot\n",
    "\n",
    "# # Fix the cutoff heatmap issue\n",
    "# b, t = plt.ylim() # discover the values for bottom and top\n",
    "# b += 0.5 # Add 0.5 to the bottom\n",
    "# t -= 0.5 # Subtract 0.5 from the top\n",
    "# plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "type(weather_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,8))\n",
    "fig.subplots_adjust(hspace=0.4,wspace=0.4)\n",
    "i=1\n",
    "for key,data in weather_date.items():\n",
    "    ax = fig.add_subplot(2,4,i)\n",
    "    sns.heatmap(data)\n",
    "    ax.set_title(key)\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create a GEOTIFF on the Lat/Long data\n",
    "\n",
    "must create geotiff first, then change crs, then upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "lats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sketches with transforms\n",
    "![DestinationTransform](DestinationTransform.jpg)\n",
    "![SourceTransform](SourceTransform.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# from https://rasterio.readthedocs.io/en/latest/quickstart.html#creating-data\n",
    "# See my imported sketches with the correct transforms\n",
    "\n",
    "#  Resolution\n",
    "xres = (longs[-1] - longs[0]) / 9\n",
    "yres = (lats[0] - lats[-1]) /9\n",
    "\n",
    "# affine transform - assumes each lat/long point is in the center of the pixel it represents\n",
    "src_transform = Affine.translation(longs[0] - xres / 2, lats[-1] - yres / 2) * Affine.scale(xres, yres)\n",
    "src_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "new_meta = {'driver':'GTiff','height':10, \\\n",
    "           'width':10,'count':1, \\\n",
    "           'dtype':'float64', 'crs':'+proj=latlong', \\\n",
    "           'transform':src_transform, 'nodata':-9999, \\\n",
    "           'compress':'lzw','interleave':'band'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Write the high temp data - change nan to -9999 first\n",
    "np.nan_to_num(HighTempArray,copy=False,nan=-9999)\n",
    "with rio.open('../data/GlobalFire2016/hitemp.tif','w', **new_meta) as new:\n",
    "    new.write(HighTempArray,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# re-open to see if it did the right thing\n",
    "with rio.open('../data/GlobalFire2016/hitemp.tif','r') as ht:\n",
    "    htmeta = ht.meta\n",
    "    ht_data = ht.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "htmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ht_data[ht_data == -9999.]='NaN'\n",
    "ht_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Reproject and upsample weather data to fire crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Use the firedata raster as the reference meta data and transform\n",
    "with rio.open('../data/GlobalFire2016/Global_fire_atlas_firelinecrop.tif','r') as ref:\n",
    "    refdata = ref.read(1)\n",
    "    refmeta = ref.meta\n",
    "    refres = ref.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dst_transform = refmeta['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dst_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "src_crs = htmeta['crs']\n",
    "dst_crs = refmeta['crs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('source crs = ',src_crs,'\\ndestination crs =',dst_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Reproject the Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(lats,longs)\n",
    "print(refmeta)\n",
    "print(refdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Change the dtype for the tempdata - fireline used integer data and we want float\n",
    "refmeta['dtype'] = 'float64'\n",
    "refmeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dst_shape = refdata.shape\n",
    "dst_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "destination = np.full(dst_shape, -9999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ht_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "src_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "src_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dst_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dst_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reproject(\n",
    "    ht_data,\n",
    "    destination,\n",
    "    src_transform=src_transform,\n",
    "    src_crs=src_crs,\n",
    "    dst_transform=dst_transform,\n",
    "    dst_crs=dst_crs,\n",
    "    resampling=Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ht_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check that raster data was projected and written correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Plot to test\n",
    "ax = sns.heatmap(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Putting it all together: Functions to get weather, create each day's weather from DarkSky json and to upsample each array then store the dictionary of arrays out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Get Weather Data Functions and call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Function to get dates of fire in pertinent area from raster file.\n",
    "def get_fire_dates(yr,path,filename):\n",
    "    \"\"\"Get all the dates for a given year that there was fire\n",
    "    in the raster being studied\n",
    "    \"\"\"\n",
    "    \n",
    "    f = path+filename\n",
    "    # Use the firedata raster to find the day of year \n",
    "    with rio.open(f,'r') as rast:\n",
    "        rastdata = rast.read(1)\n",
    "    \n",
    "    # Get unique days of the year there was a pixel on a fireline, as well as counts\n",
    "    uniq = np.unique(rastdata, return_counts=True)\n",
    "    days = uniq[0]\n",
    "    counts = uniq[1]\n",
    "    \n",
    "    # Error check that counts are greater than 0\n",
    "    if np.count_nonzero(counts) > 0:\n",
    "        days = days[days > 0]  # remove the -9999 no data values\n",
    "\n",
    "    # Create list of dates for DOY on fireline, date format = ['2016-12-16']\n",
    "    # time = date+'T12:00:00'\n",
    "    fire_dates = []\n",
    "    for day in days:\n",
    "        s = yr + '-'+ str(day)\n",
    "        d = datetime.strptime(s,'%Y-%j')\n",
    "        s2 = str(d.strftime(\"%Y-%m-%d\")) + 'T12:00:00'\n",
    "        fire_dates.append(s2)\n",
    "    \n",
    "    return fire_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fire_weather(pairs,fire_dates,key):\n",
    "    \"\"\"Function to loop through all pairs of lat/long and get the daily weather\n",
    "    for each point from DarkSky\n",
    "    Input: list of pairs of lat/long, list of dates, API Key\n",
    "    Output: Dictionary of list of daily weather jsons\n",
    "    \"\"\"\n",
    "    \n",
    "    yr_weath = {}\n",
    "    for date in fire_dates:\n",
    "        # Call function to get weather for specified date\n",
    "        data_for_date = getweather(pairs,date,key)\n",
    "        yr_weath.update({date:data_for_date})\n",
    "        \n",
    "    return yr_weath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def getweather(coordinates,date,key):\n",
    "    \"\"\" Function to take the set of coordinates and get the daily\n",
    "    weather for a given date. rain, hi and low temps, humidity\n",
    "    wind speed and direction and cloud cover only.\n",
    "    \n",
    "    Input: Date, API Key and list of coordinate pairs (lat,long)\n",
    "    Output: list of jsons\n",
    "    \"\"\"\n",
    "    # items to exclude from call\n",
    "    blocks = '[currently,minutely,hourly,alerts]'\n",
    "\n",
    "    # Units for call (km/h, deg C, kPa, mm precip)\n",
    "    units = 'ca'\n",
    "    \n",
    "    # Compression for call\n",
    "    headers = {'Accept-Encoding':'gzip'}\n",
    "    \n",
    "    data_out = []\n",
    "    for pair in coordinates:\n",
    "        lat = str(pair[0])\n",
    "        long = str(pair[1])\n",
    "        # set the query string for darksky\n",
    "        query = ('https://api.darksky.net/forecast/'+key+'/'+ \n",
    "            lat+','+long+','+date+'?exclude=' \n",
    "            +blocks+'&units='+units)\n",
    "        # Make the call to Dark Sky to get all the data for that date and location\n",
    "        try:\n",
    "            r=requests.get(query,headers=headers)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # get the weather data from the request return\n",
    "        weather=r.json()\n",
    "        \n",
    "        # write the jsons as items in the output data list\n",
    "        data_out.append(weather)\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Non url request related setup\n",
    "path = '../toydata/'\n",
    "filename = 'Global_fire_atlas_firelinecrop.tif'\n",
    "fire_dates = get_fire_dates('2016',path,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\" one time function to set limits of bounding box for weather\n",
    "    in lat / long\n",
    "    Input: None (hard coded)\n",
    "    Output: pairs of lat/long, lats and longs    \n",
    "    \"\"\"\n",
    "    # Polygon of bounding box in lat/long \n",
    "    bb_long = [-122, -119.912,-119.912,-122,-122 ]\n",
    "    bb_lat = [36.8, 36.8, 35.06, 35.06,36.8]\n",
    "\n",
    "    # Create grid of lat/long pairs for retrieving weather data\n",
    "    min_long = min(bb_long)\n",
    "    max_long = max(bb_long)\n",
    "    min_lat = min(bb_lat)\n",
    "    max_lat = max(bb_lat)\n",
    "\n",
    "    # 10 x 10 grid\n",
    "    spaces = 10\n",
    "    longs = np.linspace(min_long,max_long,spaces)\n",
    "    lats = np.linspace(min_lat,max_lat,spaces)\n",
    "    pairs = [(lat,long) for lat in lats for long in longs]\n",
    "    \n",
    "    # Reference file for raster\n",
    "    path = '../toydata/'\n",
    "    filename = 'Global_fire_atlas_firelinecrop.tif'\n",
    "    \n",
    "    return (pairs,lats,longs,path,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pairs,lats,longs,path,filename = setup()\n",
    "fire_dates = get_fire_dates('2016',path,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dark Sky API key (Laura Chutny)\n",
    "key = '5ffac5f056d341c6296cba58fa96e9ba'\n",
    "\n",
    "# trial\n",
    "# sdates = fire_dates[0:2]\n",
    "\n",
    "yr_weather = fire_weather(pairs,fire_dates,key)  # Dictionary of jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Pickle out the jsons to avoid another call:\n",
    "with open('../data/GlobalFire2016/weatherjsons.pickle','wb') as f:\n",
    "    pickle.dump(yr_weather,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('../data/GlobalFire2016/weatherjsons.pickle','rb') as f:\n",
    "    yr_weather = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2016-04-09T12:00:00', '2016-04-14T12:00:00', '2016-05-19T12:00:00', '2016-05-21T12:00:00', '2016-05-22T12:00:00', '2016-05-23T12:00:00', '2016-05-24T12:00:00', '2016-05-25T12:00:00', '2016-05-26T12:00:00', '2016-05-27T12:00:00', '2016-06-02T12:00:00', '2016-06-03T12:00:00', '2016-06-04T12:00:00', '2016-06-05T12:00:00', '2016-06-07T12:00:00', '2016-06-08T12:00:00', '2016-06-09T12:00:00', '2016-06-11T12:00:00', '2016-06-18T12:00:00', '2016-07-02T12:00:00', '2016-07-04T12:00:00', '2016-07-05T12:00:00', '2016-07-21T12:00:00', '2016-07-22T12:00:00', '2016-07-23T12:00:00', '2016-07-24T12:00:00', '2016-07-25T12:00:00', '2016-07-26T12:00:00', '2016-07-27T12:00:00', '2016-07-28T12:00:00', '2016-07-29T12:00:00', '2016-07-30T12:00:00', '2016-07-31T12:00:00', '2016-08-01T12:00:00', '2016-08-02T12:00:00', '2016-08-03T12:00:00', '2016-08-04T12:00:00', '2016-08-05T12:00:00', '2016-08-06T12:00:00', '2016-08-07T12:00:00', '2016-08-08T12:00:00', '2016-08-09T12:00:00', '2016-08-10T12:00:00', '2016-08-11T12:00:00', '2016-08-12T12:00:00', '2016-08-13T12:00:00', '2016-08-14T12:00:00', '2016-08-15T12:00:00', '2016-08-16T12:00:00', '2016-08-17T12:00:00', '2016-08-18T12:00:00', '2016-08-19T12:00:00', '2016-08-20T12:00:00', '2016-08-21T12:00:00', '2016-08-22T12:00:00', '2016-08-23T12:00:00', '2016-08-24T12:00:00', '2016-08-25T12:00:00', '2016-08-26T12:00:00', '2016-08-27T12:00:00', '2016-08-28T12:00:00', '2016-08-29T12:00:00', '2016-08-30T12:00:00', '2016-08-31T12:00:00', '2016-09-01T12:00:00', '2016-09-02T12:00:00', '2016-09-03T12:00:00', '2016-09-04T12:00:00', '2016-09-05T12:00:00', '2016-09-06T12:00:00', '2016-09-07T12:00:00', '2016-09-08T12:00:00', '2016-09-09T12:00:00', '2016-09-10T12:00:00', '2016-09-11T12:00:00', '2016-09-12T12:00:00', '2016-09-13T12:00:00', '2016-09-15T12:00:00', '2016-09-16T12:00:00', '2016-09-17T12:00:00', '2016-09-18T12:00:00', '2016-09-19T12:00:00', '2016-09-20T12:00:00', '2016-09-21T12:00:00', '2016-09-22T12:00:00', '2016-09-23T12:00:00', '2016-09-24T12:00:00', '2016-09-25T12:00:00', '2016-09-26T12:00:00', '2016-09-27T12:00:00', '2016-09-29T12:00:00', '2016-10-23T12:00:00', '2016-10-25T12:00:00', '2016-10-26T12:00:00', '2016-10-27T12:00:00', '2016-10-30T12:00:00'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr_weather.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Change jsons to dataframes and then rasters and reproject before pickling out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_point(info,rain=False):\n",
    "    \"\"\" to check if there is a value for that weather element and provide \n",
    "    either None or the value back\n",
    "    \"\"\"\n",
    "    if info:\n",
    "        return info\n",
    "    elif (not info and rain):\n",
    "        return 0.0        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def date_df(data_out):\n",
    "    \"\"\" Function to take the list of jsons for each lat/long from one date\n",
    "    of daily weather request and create a pandas dataframe\n",
    "    \n",
    "    Input: list of jsons containing the weather data for one day\n",
    "    Output: pandas dataframe\n",
    "    \"\"\"\n",
    "    data_convert=[]\n",
    "    # Titles for the data\n",
    "    coltitles = ['date','latitude','longitude','rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']\n",
    "    data_convert.append(coltitles)\n",
    "    # Loope through all the geopoints to get the data\n",
    "    for point in data_out:\n",
    "        lat = point['latitude']\n",
    "        long = point['longitude']\n",
    "        daily = point.get('daily')\n",
    "        if daily:\n",
    "            data = daily['data'][0]\n",
    "            date = datetime.fromtimestamp(data['time']).strftime('%Y-%m-%d')\n",
    "            rainint = data.get('precipIntensityMax')\n",
    "            raintot = data.get('precipAccumulation')\n",
    "            hitemp = data.get('temperatureHigh')\n",
    "            lotemp = data.get('temperatureLow')\n",
    "            humidity = data.get('humidity')\n",
    "            windspd = data.get('windSpeed')\n",
    "            winddir = data.get('windBearing')\n",
    "            clouds = data.get('cloudCover')\n",
    "            point = [date,lat,long,check_point(rainint,1),check_point(raintot,1), \\\n",
    "                     check_point(hitemp,0),check_point(lotemp,0), \\\n",
    "                     check_point(humidity,0),check_point(windspd,0), \\\n",
    "                     check_point(winddir,0), check_point(clouds,0)] \n",
    "            data_convert.append(point)\n",
    "            \n",
    "        else:\n",
    "            point = None\n",
    "            \n",
    "    df = pd.DataFrame(data_convert[1:], columns = data_convert[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def jsons_to_arrays(date_jsons,pairs):\n",
    "    \"\"\" Take list of jsons for weather for area being studied (100 jsons)\n",
    "    for 1 day and turn them into a dictionary of numpy arrays\n",
    "    \n",
    "    Input: list of jsons - 1 per lat/long point and pairs of lat/long\n",
    "    Output: dictionary of numpy arrays - 1 for each weather type for given date\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input jsons to pandas dataframe:\n",
    "    df = date_df(date_jsons)\n",
    "          \n",
    "    # Add Missing Rows to dataframe:\n",
    "    # First get a paired list to determine if any rows are missing\n",
    "    df['xx'] = list(zip(df['latitude'],df['longitude']))\n",
    "    missing = pd.DataFrame({'xx':list(set(df['xx']) ^ set(pairs))})\n",
    "    if not missing.empty:\n",
    "        missing[['latitude','longitude']]=pd.DataFrame(missing['xx'].tolist(),index=missing.index)\n",
    "        df = df.append(missing, ignore_index=True, sort=True)\n",
    "    \n",
    "    # Sort DataFrame to allow creation of rows in array.\n",
    "    weather = ['rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']\n",
    "    weath_sort = df.to_records(index=False)\n",
    "    weath_sort.sort(order=('latitude','longitude'))\n",
    "\n",
    "    # Create Dictionary of Numpy Arrays\n",
    "    weather_date = {}\n",
    "    for var in weather:\n",
    "        x = weath_sort[var].reshape(10,10)\n",
    "        xx = np.flip(x,0)\n",
    "        weather_date.update({var:xx})\n",
    "        \n",
    "    return weather_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def new_meta(lats,longs):\n",
    "    \"\"\" Calculate transform and meta data for 10x10 \n",
    "    raster of lat/long data\n",
    "    \"\"\"\n",
    "    \n",
    "    # from https://rasterio.readthedocs.io/en/latest/quickstart.html#creating-data\n",
    "    # See my imported sketches with the correct transforms\n",
    "    \n",
    "    #  Resolution\n",
    "    xres = (longs[-1] - longs[0]) / 9\n",
    "    yres = (lats[0] - lats[-1]) /9\n",
    "    \n",
    "    # affine transform - assumes each lat/long point is in the center of the pixel it represents\n",
    "    src_transform = Affine.translation(longs[0] - xres / 2, lats[-1] - yres / 2) * Affine.scale(xres, yres)\n",
    "    \n",
    "    meta = {'driver':'GTiff','height':10, \\\n",
    "                'width':10,'count':1, \\\n",
    "                'dtype':'float64', 'crs':'+proj=latlong', \\\n",
    "                'transform':src_transform, 'nodata':-9999, \\\n",
    "                'compress':'lzw','interleave':'band'}\n",
    "    \n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reproject_arrs(arrs,path,filename,src_meta):\n",
    "    \"\"\"Reproject the lat/long weather arrays to\n",
    "    raster arrays with new CRS\n",
    "    \n",
    "    Input: Dictionary of numpy arrays holding 8 weather variables\n",
    "    for one date in 10x10 grid based on lat/long, path and filename to raster\n",
    "    and Source meta data\n",
    "    Output: Dictionary of numpy arrays holding 8 weather variables\n",
    "    for one date in grid to match firedata raster.\n",
    "    \"\"\"    \n",
    "    # Get reference meta data from raster\n",
    "    file = path+filename\n",
    "    with rio.open(file,'r') as ref:\n",
    "        refdata = ref.read(1)  # data in raster\n",
    "        refmeta = ref.meta # meta data in raster\n",
    "        refres = ref.res # resolution in raster\n",
    "    \n",
    "    # Set destination transform and coordinate reference system\n",
    "    dst_transform = refmeta['transform']\n",
    "    dst_crs = refmeta['crs']\n",
    "\n",
    "    # Get source transform and CRS\n",
    "    src_transform = src_meta['transform']\n",
    "    src_crs = src_meta['crs']\n",
    "    \n",
    "    refmeta['dtype'] = 'float64'\n",
    "    \n",
    "    dst_shape = refdata.shape\n",
    "    destination = np.full(dst_shape, -9999.0)\n",
    "    \n",
    "    rp = {}\n",
    "    for w,a in arrs.items():\n",
    "        # clear the array before reprojecting each time\n",
    "        destination = np.full(dst_shape, -9999.0)\n",
    "        \n",
    "        reproject(\n",
    "            a,\n",
    "            destination,\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=dst_transform,\n",
    "            dst_crs=dst_crs,\n",
    "            resampling=Resampling.bilinear)\n",
    "        \n",
    "        rp.update({str(w):destination})\n",
    "\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create reprojected arrays for each date and pickle out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create function to loop over all dates and write out the dictionary\n",
    "def all_weather(yr_weather):\n",
    "    \"\"\"Loop through all the dates for fire in a year and\n",
    "    corresponding daily weather jsons (yr_weather), convert\n",
    "    to numpy arrays and reproject the arrays, the save out the new dictionary\n",
    "    \n",
    "    Input: yearly weather from darksky as date keyed dictionary of jsons\n",
    "    Output: write out the reprojected yearly weather arrays as pickle file\n",
    "    \"\"\"\n",
    "    # Get pairs of lat / long and path and filename for raster reference file\n",
    "    pairs,lats,longs,path,filename = setup()\n",
    "    \n",
    "    yr_rp_weather = {}    \n",
    "    for date,data in yr_weather.items():\n",
    "        # Convert jsons to arrays for given date\n",
    "        day_data = jsons_to_arrays(data,pairs)\n",
    "        \n",
    "        # Reproject arrays\n",
    "        reproj = reproject_arrs(day_data,path,filename,new_meta(lats,longs))\n",
    "        dt = date[0:10]\n",
    "        \n",
    "        yr_rp_weather.update({dt:reproj})\n",
    "        \n",
    "    # Pickle out the reprojected array dictionary:\n",
    "    with open('../data/GlobalFire2016/weather2016data.pickle','wb') as f:\n",
    "        pickle.dump(yr_rp_weather,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return (len(yr_rp_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days of weather reprojected for 2016: 96\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weather_days = all_weather(yr_weather)\n",
    "print('Number of days of weather reprojected for 2016:',weather_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if all_weather worked correctly.\n",
    "with open('../data/GlobalFire2016/weather2016data.pickle','rb') as f:\n",
    "    y1 = pickle.load(f)   \n",
    "    \n",
    "len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2016-04-09', '2016-04-14', '2016-05-19', '2016-05-21', '2016-05-22', '2016-05-23', '2016-05-24', '2016-05-25', '2016-05-26', '2016-05-27', '2016-06-02', '2016-06-03', '2016-06-04', '2016-06-05', '2016-06-07', '2016-06-08', '2016-06-09', '2016-06-11', '2016-06-18', '2016-07-02', '2016-07-04', '2016-07-05', '2016-07-21', '2016-07-22', '2016-07-23', '2016-07-24', '2016-07-25', '2016-07-26', '2016-07-27', '2016-07-28', '2016-07-29', '2016-07-30', '2016-07-31', '2016-08-01', '2016-08-02', '2016-08-03', '2016-08-04', '2016-08-05', '2016-08-06', '2016-08-07', '2016-08-08', '2016-08-09', '2016-08-10', '2016-08-11', '2016-08-12', '2016-08-13', '2016-08-14', '2016-08-15', '2016-08-16', '2016-08-17', '2016-08-18', '2016-08-19', '2016-08-20', '2016-08-21', '2016-08-22', '2016-08-23', '2016-08-24', '2016-08-25', '2016-08-26', '2016-08-27', '2016-08-28', '2016-08-29', '2016-08-30', '2016-08-31', '2016-09-01', '2016-09-02', '2016-09-03', '2016-09-04', '2016-09-05', '2016-09-06', '2016-09-07', '2016-09-08', '2016-09-09', '2016-09-10', '2016-09-11', '2016-09-12', '2016-09-13', '2016-09-15', '2016-09-16', '2016-09-17', '2016-09-18', '2016-09-19', '2016-09-20', '2016-09-21', '2016-09-22', '2016-09-23', '2016-09-24', '2016-09-25', '2016-09-26', '2016-09-27', '2016-09-29', '2016-10-23', '2016-10-25', '2016-10-26', '2016-10-27', '2016-10-30'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rainint': array([[0.48923632, 0.48816009, 0.48708386, ..., 5.2302676 , 5.11784022,\n",
       "         5.00541284],\n",
       "        [0.48186621, 0.48078995, 0.47971369, ..., 5.20568663, 5.0934345 ,\n",
       "         4.98118237],\n",
       "        [0.47449576, 0.47341946, 0.47234316, ..., 5.18110448, 5.06902761,\n",
       "         4.95695074],\n",
       "        ...,\n",
       "        [       nan,        nan,        nan, ..., 2.6364685 , 2.67762072,\n",
       "         2.71877295],\n",
       "        [       nan,        nan,        nan, ..., 2.64920797, 2.69143312,\n",
       "         2.73365827],\n",
       "        [       nan,        nan,        nan, ..., 2.65668383, 2.69953859,\n",
       "         2.74239336]]),\n",
       " 'raintot': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan, nan, ...,  0.,  0.,  0.],\n",
       "        [nan, nan, nan, ...,  0.,  0.,  0.],\n",
       "        [nan, nan, nan, ...,  0.,  0.,  0.]]),\n",
       " 'High T': array([[17.20008049, 17.19940659, 17.19873269, ..., 18.93318837,\n",
       "         18.92883479, 18.9244812 ],\n",
       "        [17.20060061, 17.19976184, 17.19892307, ..., 18.93816827,\n",
       "         18.93379595, 18.92942363],\n",
       "        [17.20112076, 17.20011711, 17.19911346, ..., 18.9431484 ,\n",
       "         18.93875735, 18.93436629],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ..., 15.71749965,\n",
       "         15.73184506, 15.74619048],\n",
       "        [        nan,         nan,         nan, ..., 15.72215931,\n",
       "         15.73642429, 15.75068928],\n",
       "        [        nan,         nan,         nan, ..., 15.72489372,\n",
       "         15.7391115 , 15.75332929]]),\n",
       " 'Low T': array([[12.94150372, 12.94088468, 12.94026564, ..., 12.80455987,\n",
       "         12.79291845, 12.78127703],\n",
       "        [12.94478416, 12.94371172, 12.94263929, ..., 12.81627843,\n",
       "         12.80465574, 12.79303306],\n",
       "        [12.94806476, 12.9465389 , 12.94501305, ..., 12.82799755,\n",
       "         12.8163936 , 12.80478965],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ..., 10.45439505,\n",
       "         10.47784836, 10.50130167],\n",
       "        [        nan,         nan,         nan, ..., 10.45948046,\n",
       "         10.48283802, 10.50619557],\n",
       "        [        nan,         nan,         nan, ..., 10.46246471,\n",
       "         10.48576608, 10.50906744]]),\n",
       " 'Humidity': array([[0.87045797, 0.87105037, 0.87164278, ..., 0.84079674, 0.84039809,\n",
       "         0.83999945],\n",
       "        [0.87102723, 0.87161964, 0.87221205, ..., 0.84043573, 0.84002959,\n",
       "         0.83962346],\n",
       "        [0.87159652, 0.87218893, 0.87278134, ..., 0.84007471, 0.83966108,\n",
       "         0.83924745],\n",
       "        ...,\n",
       "        [       nan,        nan,        nan, ..., 0.8639007 , 0.86232094,\n",
       "         0.86074119],\n",
       "        [       nan,        nan,        nan, ..., 0.8639007 , 0.86232094,\n",
       "         0.86074119],\n",
       "        [       nan,        nan,        nan, ..., 0.8639007 , 0.86232094,\n",
       "         0.86074119]]),\n",
       " 'Wind Speed': array([[3.8477221 , 3.8544342 , 3.8611463 , ..., 6.66050411, 6.6595927 ,\n",
       "         6.65868129],\n",
       "        [3.85929605, 3.8660044 , 3.87271276, ..., 6.67682323, 6.67606545,\n",
       "         6.67530767],\n",
       "        [3.87087055, 3.87757516, 3.88427977, ..., 6.69314313, 6.69253899,\n",
       "         6.69193484],\n",
       "        ...,\n",
       "        [       nan,        nan,        nan, ..., 6.1648322 , 6.17521309,\n",
       "         6.18559398],\n",
       "        [       nan,        nan,        nan, ..., 6.15871991, 6.16890546,\n",
       "         6.17909102],\n",
       "        [       nan,        nan,        nan, ..., 6.15513305, 6.16520398,\n",
       "         6.17527491]]),\n",
       " 'Wind Direction': array([[260.23085194, 260.15575366, 260.08065539, ..., 333.03302071,\n",
       "         332.66023695, 332.28745319],\n",
       "        [260.74553658, 260.67830718, 260.61107778, ..., 333.24869906,\n",
       "         332.88078651, 332.51287395],\n",
       "        [261.26024588, 261.20088572, 261.14152556, ..., 333.46438774,\n",
       "         333.10134662, 332.7383055 ],\n",
       "        ...,\n",
       "        [         nan,          nan,          nan, ..., 203.05443314,\n",
       "         203.62435328, 204.19427342],\n",
       "        [         nan,          nan,          nan, ..., 202.72850284,\n",
       "         203.31259434, 203.89668583],\n",
       "        [         nan,          nan,          nan, ..., 202.53723842,\n",
       "         203.12964604, 203.72205365]]),\n",
       " 'Cloud Cover': array([[0.89927194, 0.89966133, 0.90005071, ..., 0.82488848, 0.82684466,\n",
       "         0.82880083],\n",
       "        [0.8975608 , 0.89793894, 0.89831708, ..., 0.82441174, 0.82633044,\n",
       "         0.82824914],\n",
       "        [0.89584957, 0.89621647, 0.89658337, ..., 0.82393497, 0.8258162 ,\n",
       "         0.82769743],\n",
       "        ...,\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan]])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 =y1['2016-04-09']\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "w1 = x1['High T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 456)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Determine which days of the year we need data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Use the firedata raster as the reference meta data and transform\n",
    "with rio.open('../toydata/Global_fire_atlas_firelinecrop.tif','r') as rast:\n",
    "    rastdata = rast.read(1)\n",
    "    rastmeta = rast.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "rastdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "uniq = np.unique(rastdata, return_counts=True)\n",
    "days = uniq[0]\n",
    "counts = uniq[1]\n",
    "# days = days[days > 0]  # remove the -9999 no data values\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(len(days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Cost for 2016:\n",
    "nopoints = 10*10 # weather data on 10x10 grid\n",
    "nodays = len(days) # number of unique days \n",
    "total_calls = nopoints * nodays\n",
    "percall = 0.0001 # $0.0001 USD/call\n",
    "free = 1000 # number free calls per day\n",
    "cost = (total_calls - free)*percall\n",
    "print(f\"Cost to obtain data for all 2016 for cropped area is: ${cost} for {total_calls} total calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Find date for given day of year\n",
    "datetime.strptime('2016-100','%Y-%j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create list of dates for DOY on fireline\n",
    "# dates = ['2016-12-16']\n",
    "# time = date+'T12:00:00'\n",
    "fire_dates = []\n",
    "yr = '2016'\n",
    "for day in days:\n",
    "    s = yr + '-'+ str(day)\n",
    "    d = datetime.strptime(s,'%Y-%j')\n",
    "    s2 = str(d.strftime(\"%Y-%m-%d\")) + 'T12:00:00'\n",
    "    fire_dates.append(s2)\n",
    "len(fire_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check if dates above cover the dates for the ignitions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('../toydata/Global_fire_atlas_datacrop.pickle','rb') as f:\n",
    "    firedbf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ign_dates = firedbf['start_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ign_dates2 = []\n",
    "for day in ign_dates:\n",
    "    s3 = day + 'T12:00:00'\n",
    "    ign_dates2.append(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Check if ignition dates amongst the fireline dates\n",
    "missed = set(ign_dates2).issubset(set(fire_dates))\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The ignition days are also within the fireline days, so just pickle out the arrays for each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Sa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
