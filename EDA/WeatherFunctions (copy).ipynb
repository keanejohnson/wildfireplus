{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import requests\n",
    "import faster_than_requests as ftreq\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"matplotlib\",matplotlib.__version__)\n",
    "print(\"seaborn\",sns.__version__)\n",
    "print(\"numpy\",np.__version__)\n",
    "print(\"pandas\",pd.__version__)\n",
    "print(\"python\",sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Forecast Data API supports HTTP compression. We heartily recommend using it, as it will make responses much smaller over the wire. To enable it, simply add an `Accept-Encoding: gzip` header to your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: Functions to get weather, create each day's weather from DarkSky json and to upsample each array then store the dictionary of arrays out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weather Data Functions and call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get dates of fire in pertinent area from raster file.\n",
    "def get_fire_dates(yr,path,filename):\n",
    "    \"\"\"Get all the dates for a given year that there was fire\n",
    "    in the raster being studied\n",
    "    \"\"\"\n",
    "    import rasterio as rio\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    f = path+filename\n",
    "    # Use the firedata raster to find the day of year \n",
    "    with rio.open(f,'r') as rast:\n",
    "        rastdata = rast.read(1)\n",
    "    \n",
    "    # Get unique days of the year there was a pixel on a fireline, as well as counts\n",
    "    uniq = np.unique(rastdata, return_counts=True)\n",
    "    days = uniq[0]\n",
    "    counts = uniq[1]\n",
    "    \n",
    "    # Error check that counts are greater than 0\n",
    "    if np.count_nonzero(counts) > 0:\n",
    "        days = days[days > 0]  # remove the -9999 no data values\n",
    "\n",
    "    # Create list of dates for DOY on fireline, date format = ['2016-12-16']\n",
    "    # time = date+'T12:00:00'\n",
    "    fire_dates = []\n",
    "    for day in days:\n",
    "        s = yr + '-'+ str(day)\n",
    "        d = datetime.strptime(s,'%Y-%j')\n",
    "        s2 = str(d.strftime(\"%Y-%m-%d\")) + 'T12:00:00'\n",
    "        fire_dates.append(s2)\n",
    "    \n",
    "    return fire_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_weather(pairs,fire_dates,key):\n",
    "    \"\"\"Function to loop through all pairs of lat/long and get the daily weather\n",
    "    for each point from DarkSky\n",
    "    Input: list of pairs of lat/long, list of dates, API Key\n",
    "    Output: Dictionary of list of daily weather jsons\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    s = requests.Session()\n",
    "    s.auth = ('user', 'pass')\n",
    "    s.headers.update({'Accept-Encoding':'gzip'})\n",
    "\n",
    "    \n",
    "    yr_weath = {}\n",
    "    for date in fire_dates:\n",
    "        # Call function to get weather for specified date\n",
    "        data_for_date = getweather(s,pairs,date,key)\n",
    "        yr_weath.update({date:data_for_date})\n",
    "        \n",
    "    return yr_weath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweather(s,coordinates,date,key):\n",
    "    \"\"\" Function to take the set of coordinates and get the daily\n",
    "    weather for a given date. rain, hi and low temps, humidity\n",
    "    wind speed and direction and cloud cover only.\n",
    "    \n",
    "    Called from fire_weather\n",
    "    \n",
    "    Input: Date, API Key and list of coordinate pairs (lat,long)\n",
    "    Output: list of jsons\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    # items to exclude from call\n",
    "    blocks = '[currently,minutely,hourly,alerts]'\n",
    "\n",
    "    # Units for call (km/h, deg C, kPa, mm precip)\n",
    "    units = 'ca'\n",
    "    \n",
    "    # Compression for call\n",
    "    headers = {'Accept-Encoding':'gzip'}\n",
    "    \n",
    "    data_out = []\n",
    "    for pair in coordinates:\n",
    "        lat = str(pair[0])\n",
    "        long = str(pair[1])\n",
    "        # set the query string for darksky\n",
    "        query = ('https://api.darksky.net/forecast/'+key+'/'+ \n",
    "            lat+','+long+','+date+'?exclude=' \n",
    "            +blocks+'&units='+units)\n",
    "        # Make the call to Dark Sky to get all the data for that date and location\n",
    "        r=s.get(query,headers=headers)\n",
    "        \n",
    "        # get the weather data from the request return\n",
    "        weather=r.json()\n",
    "        \n",
    "        # write the jsons as items in the output data list\n",
    "        data_out.append(weather)\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getweather(coordinates,date,key):\n",
    "#     \"\"\" Function to take the set of coordinates and get the daily\n",
    "#     weather for a given date. rain, hi and low temps, humidity\n",
    "#     wind speed and direction and cloud cover only.\n",
    "    \n",
    "#     Called from fire_weather\n",
    "    \n",
    "#     Input: Date, API Key and list of coordinate pairs (lat,long)\n",
    "#     Output: list of jsons\n",
    "#     \"\"\"\n",
    "#     import faster_than_requests as ftreq\n",
    "#     import json\n",
    "    \n",
    "#     # items to exclude from call\n",
    "#     blocks = '[currently,minutely,hourly,alerts]'\n",
    "\n",
    "#     # Units for call (km/h, deg C, kPa, mm precip)\n",
    "#     units = 'ca'\n",
    "    \n",
    "#     data_out = []\n",
    "#     for pair in coordinates:\n",
    "#         lat = str(pair[0])\n",
    "#         long = str(pair[1])\n",
    "#         # set the query string for darksky\n",
    "#         query = ('https://api.darksky.net/forecast/'+key+'/'+ \n",
    "#             lat+','+long+','+date+'?exclude=' \n",
    "#             +blocks+'&units='+units)\n",
    "#         # Make the call to Dark Sky to get all the data for that date and location\n",
    "#         ftreq.set_headers(headers = [('Accept-Encoding','gzip')])\n",
    "#         r=ftreq.get(query,headers=headers)\n",
    "\n",
    "#         # get the weather data from the request return\n",
    "#         weather=r.json()\n",
    "        \n",
    "#         # write the jsons as items in the output data list\n",
    "#         data_out.append(weather)\n",
    "    \n",
    "#     return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(path,filename):\n",
    "    \"\"\" one time function to set limits of bounding box for weather\n",
    "    in lat / long\n",
    "    Input: raster\n",
    "    Output: pairs of lat/long, lats and longs    \n",
    "    \"\"\"\n",
    "    import rasterio as rio\n",
    "    import numpy as np\n",
    "    \n",
    "    filestring = path+filename\n",
    "    with rio.open(filestring) as f:\n",
    "        left,bottom,right,top = f.bounds\n",
    "    \n",
    "    # for rectangle\n",
    "    bb_long = [left,right,right,left,left]\n",
    "    bb_lat = [top,top,bottom,bottom,top]\n",
    "    \n",
    "    # Create grid of lat/long pairs for retrieving weather data\n",
    "    min_long = min(bb_long)\n",
    "    max_long = max(bb_long)\n",
    "    min_lat = min(bb_lat)\n",
    "    max_lat = max(bb_lat)\n",
    "\n",
    "    # 15 x 15 grid\n",
    "    spaces = 15\n",
    "    longs = np.linspace(min_long,max_long,spaces)\n",
    "    lats = np.linspace(min_lat,max_lat,spaces)\n",
    "    pairs = [(lat,long) for lat in lats for long in longs]\n",
    "    \n",
    "    return (pairs,lats,longs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Change jsons to dataframes and then rasters and reproject before pickling out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_point(info,rain=False):\n",
    "    \"\"\" to check if there is a value for that weather element and provide \n",
    "    either None or the value back\n",
    "    Used in function date_df only\n",
    "    \"\"\"\n",
    "    if info:\n",
    "        return info\n",
    "    elif (not info and rain):\n",
    "        return 0.0        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_df(data_out):\n",
    "    \"\"\" Function to take the list of jsons for each lat/long from one date\n",
    "    of daily weather request and create a pandas dataframe\n",
    "    \n",
    "    Input: list of jsons containing the weather data for one day\n",
    "    Output: pandas dataframe\n",
    "    \"\"\"\n",
    "    data_convert=[]\n",
    "    # Titles for the data\n",
    "    coltitles = ['date','latitude','longitude','rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']\n",
    "    data_convert.append(coltitles)\n",
    "    # Loope through all the geopoints to get the data\n",
    "    for point in data_out:\n",
    "        lat = point['latitude']\n",
    "        long = point['longitude']\n",
    "        daily = point.get('daily')\n",
    "        if daily:\n",
    "            data = daily['data'][0]\n",
    "            date = datetime.fromtimestamp(data['time']).strftime('%Y-%m-%d')\n",
    "            rainint = data.get('precipIntensityMax')\n",
    "            raintot = data.get('precipAccumulation')\n",
    "            hitemp = data.get('temperatureHigh')\n",
    "            lotemp = data.get('temperatureLow')\n",
    "            humidity = data.get('humidity')\n",
    "            windspd = data.get('windSpeed')\n",
    "            winddir = data.get('windBearing')\n",
    "            clouds = data.get('cloudCover')\n",
    "            point = [date,lat,long,check_point(rainint,1),check_point(raintot,1), \\\n",
    "                     check_point(hitemp,0),check_point(lotemp,0), \\\n",
    "                     check_point(humidity,0),check_point(windspd,0), \\\n",
    "                     check_point(winddir,0), check_point(clouds,0)] \n",
    "            data_convert.append(point)\n",
    "            \n",
    "        else:\n",
    "            point = None\n",
    "            \n",
    "    df = pd.DataFrame(data_convert[1:], columns = data_convert[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsons_to_arrays(date_jsons,pairs):\n",
    "    \"\"\" Take list of jsons for weather for area being studied (YxY jsons)\n",
    "    for 1 day and turn them into a dictionary of numpy arrays\n",
    "    \n",
    "    Input: list of jsons - 1 per lat/long point and pairs of lat/long\n",
    "    Output: dictionary of numpy arrays - 1 for each weather type for given date\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input jsons to pandas dataframe:\n",
    "    df = date_df(date_jsons)\n",
    "          \n",
    "    # Add Missing Rows to dataframe:\n",
    "    # First get a paired list to determine if any rows are missing\n",
    "    df['xx'] = list(zip(df['latitude'],df['longitude']))\n",
    "    missing = pd.DataFrame({'xx':list(set(df['xx']) ^ set(pairs))})\n",
    "    if not missing.empty:\n",
    "        missing[['latitude','longitude']]=pd.DataFrame(missing['xx'].tolist(),index=missing.index)\n",
    "        df = df.append(missing, ignore_index=True, sort=True)\n",
    "    \n",
    "    # Sort DataFrame to allow creation of rows in array.\n",
    "    weather = ['rainint','raintot','High T','Low T','Humidity','Wind Speed','Wind Direction','Cloud Cover']\n",
    "    weath_sort = df.to_records(index=False)\n",
    "    weath_sort.sort(order=('latitude','longitude'))\n",
    "\n",
    "    # Create Dictionary of Numpy Arrays\n",
    "    weather_date = {}\n",
    "    for var in weather:\n",
    "        x = weath_sort[var].reshape(20,20)\n",
    "        xx = np.flip(x,0)\n",
    "        weather_date.update({var:xx})\n",
    "        \n",
    "    return weather_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_meta(lats,longs):\n",
    "    \"\"\" Calculate transform and meta data for 20x20 \n",
    "    raster of lat/long data\n",
    "    \"\"\"\n",
    "    \n",
    "    # from https://rasterio.readthedocs.io/en/latest/quickstart.html#creating-data\n",
    "    # See my imported sketches with the correct transforms\n",
    "    \n",
    "    #  Resolution\n",
    "    xres = (longs[-1] - longs[0]) / 19\n",
    "    yres = (lats[0] - lats[-1]) /19\n",
    "    \n",
    "    # affine transform - assumes each lat/long point is in the center of the pixel it represents\n",
    "    src_transform = Affine.translation(longs[0] - xres / 2, lats[-1] - yres / 2) * Affine.scale(xres, yres)\n",
    "    \n",
    "    meta = {'driver':'GTiff','height':20, \\\n",
    "                'width':20,'count':1, \\\n",
    "                'dtype':'float64', 'crs':'+proj=latlong', \\\n",
    "                'transform':src_transform, 'nodata':-9999, \\\n",
    "                'compress':'lzw','interleave':'band'}\n",
    "    \n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_arrs(arrs,path,filename,src_meta):\n",
    "    \"\"\"Reproject the lat/long weather arrays to\n",
    "    raster arrays with new CRS\n",
    "    \n",
    "    Input: Dictionary of numpy arrays holding 8 weather variables\n",
    "    for one date in 20x20 grid based on lat/long, path and filename to raster\n",
    "    and Source meta data\n",
    "    Output: Dictionary of numpy arrays holding 8 weather variables\n",
    "    for one date in grid to match firedata raster.\n",
    "    \"\"\"    \n",
    "    # Get reference meta data from raster\n",
    "    file = path+filename\n",
    "    with rio.open(file,'r') as ref:\n",
    "        refdata = ref.read(1)  # data in raster\n",
    "        refmeta = ref.meta # meta data in raster\n",
    "        refres = ref.res # resolution in raster\n",
    "    \n",
    "    # Set destination transform and coordinate reference system\n",
    "    dst_transform = refmeta['transform']\n",
    "    dst_crs = refmeta['crs']\n",
    "\n",
    "    # Get source transform and CRS\n",
    "    src_transform = src_meta['transform']\n",
    "    src_crs = src_meta['crs']\n",
    "    \n",
    "    refmeta['dtype'] = 'float64'\n",
    "    \n",
    "    dst_shape = refdata.shape\n",
    "    destination = np.full(dst_shape, -9999.0)\n",
    "    \n",
    "    rp = {}\n",
    "    for w,a in arrs.items():\n",
    "        # clear the array before reprojecting each time\n",
    "        destination = np.full(dst_shape, -9999.0)\n",
    "        \n",
    "        reproject(\n",
    "            a,\n",
    "            destination,\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=dst_transform,\n",
    "            dst_crs=dst_crs,\n",
    "            resampling=Resampling.bilinear)\n",
    "        \n",
    "        rp.update({str(w):destination})\n",
    "\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Create reprojected arrays for each date and pickle out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to loop over all dates and write out the dictionary\n",
    "def all_weather(yr_weather,pairs,lats,longs):\n",
    "    \"\"\"Loop through all the dates for fire in a year and\n",
    "    corresponding daily weather jsons (yr_weather), convert\n",
    "    to numpy arrays and reproject the arrays, then save out the new dictionary\n",
    "    \n",
    "    Input: yearly weather from darksky as date keyed dictionary of jsons, with lat/long pairs as pairs and separate lists\n",
    "    Output: write out the reprojected yearly weather arrays as pickle file\n",
    "    \"\"\"\n",
    "    \n",
    "    yr_rp_weather = {}    \n",
    "    for date,data in yr_weather.items():\n",
    "        # Convert jsons to arrays for given date\n",
    "        day_data = jsons_to_arrays(data,pairs)  #dictionary of numpy arrays - 1 array for each weather variable keyed by date\n",
    "        \n",
    "        # Reproject arrays\n",
    "        reproj = reproject_arrs(day_data,path,filename,new_meta(lats,longs))\n",
    "        dt = date[0:10]\n",
    "        \n",
    "        yr_rp_weather.update({dt:reproj})\n",
    "        \n",
    "    # Pickle out the reprojected array dictionary:\n",
    "    with open('../data/GlobalFire2016/weather2016data.pickle','wb') as f:\n",
    "        pickle.dump(yr_rp_weather,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return (len(yr_rp_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the weather data for each year from darksky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(year):\n",
    "    \"\"\"Function to run weather functions to get weather data from DarkSky\n",
    "    for each day in the given year there is a fire in a pixel in the area under question\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # Non-Request related setup\n",
    "    path = '../data/GlobalFire/'+year+'/'\n",
    "    filename = 'BBfirelineCA.tif'  # use fireline to get all pixels involved in a fire in that year\n",
    "\n",
    "    # Get pairs of lats and longs\n",
    "    pairs,lats,longs = setup(path,filename)\n",
    "\n",
    "    # Get dates for required request\n",
    "    fire_dates = get_fire_dates(year,path,filename)\n",
    "\n",
    "    # Dark Sky API key (Laura Chutny)\n",
    "    key = '5ffac5f056d341c6296cba58fa96e9ba'\n",
    "    # Make requests to DarkSky\n",
    "    yr_weather = fire_weather(pairs,fire_dates,key)  # Dictionary of jsons\n",
    "\n",
    "    # Pickle out the jsons to avoid another call to API (just in case something goes wrong)\n",
    "    weathfile = path+'weatherjsons.pickle'\n",
    "    with open(weathfile,'wb') as f:\n",
    "        pickle.dump(yr_weather,f,pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"{weathfile} printed for year {year}\")\n",
    "\n",
    "#     weather_days = all_weather(yr_weather,pairs,lats,longs)\n",
    "#     print(f\"Number of days of weather reprojected for {year}:{weather_days}\")\n",
    "    \n",
    "    return fire_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/GlobalFire/2009/weatherjsons.pickle printed for year 2009\n",
      "../data/GlobalFire/2010/weatherjsons.pickle printed for year 2010\n",
      "CPU times: user 6min 32s, sys: 13.1 s, total: 6min 46s\n",
      "Wall time: 1h 49min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#years = ['2003']\n",
    "#years = ['2004','2005','2006','2007']\n",
    "#years = ['2008']\n",
    "years = ['2009','2010']\n",
    "#years =['2011','2012']\n",
    "#years = ['2013','2014','2015','2016']\n",
    "fire_dates={}\n",
    "fire_date_nums={}\n",
    "for year in years:\n",
    "    fds = weather(year)\n",
    "    fire_dates[year]=fds\n",
    "    fire_date_nums[year]=len(fds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of API calls\n",
    "num_points_day = 15*15  #20x20 matrix of lat long\n",
    "# count total days:\n",
    "totaldays = 0\n",
    "for year in fire_date_nums:\n",
    "    totaldays += fire_date_nums[year]\n",
    "print(totaldays)\n",
    "total_calls=totaldays*num_points_day\n",
    "print(total_calls)\n",
    "cost_percall = 0.0001\n",
    "totalcost = total_calls*cost_percall\n",
    "print(f\"${totalcost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to reproject and save weather Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject the data and save out as a pickled numpy array.\n",
    "FIX pickle out file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#years = ['2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016',]\n",
    "years = ['2016']\n",
    "weather(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if data correctly reprojected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weather(day_weath):\n",
    "    \"\"\" Plot all the weather components for that day\"\"\"\n",
    "    fig = plt.figure(figsize = (20,8))\n",
    "    fig.subplots_adjust(hspace=0.4,wspace=0.4)\n",
    "    i=1\n",
    "    for key,data in day_weath.items():\n",
    "        ax = fig.add_subplot(2,4,i)\n",
    "        sns.heatmap(data)\n",
    "        ax.set_title(key)\n",
    "        i+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = y1[0]\n",
    "plot_weather(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    #path = '../data/GlobalFire/'+year+'/'   \n",
    "    #filename = 'weather'+year+'data.pickle'\n",
    "    #filestring = path+filename\n",
    "    filestring='../data/GlobalFire2016/weather2016data.pickle'\n",
    "    with open(filestring,'rb') as f:\n",
    "        y1 = pickle.load(f)\n",
    "    x1 = y[1] #arbitrary day\n",
    "    print(f\"Year{year}\")\n",
    "    plot_weather(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return from pickle for further work\n",
    "import pickle\n",
    "with open('../data/GlobalFire/2003/weatherjsons.pickle','rb') as f:\n",
    "    z = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['2003-01-10T12:00:00']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
