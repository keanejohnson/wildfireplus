{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-CNN Models for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes fire-specific data saved in tif files or pickle files, along with weather data covering the period the fire burned, and produces data that can tain a CNN. The data is saved to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import boto3\n",
    "import io\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## VARIABLES ##\n",
    "###############\n",
    "\n",
    "# s3 config\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'hotzone'\n",
    "\n",
    "# name of directory with fire tif files\n",
    "tif_directory = \"data\"\n",
    "\n",
    "# name of directory with weather data\n",
    "weather_directory = 'weather_data'\n",
    "\n",
    "# name of fire direction file\n",
    "direction_file = 'BBdirectionCA'\n",
    "\n",
    "# name of fire speed file\n",
    "speed_file = 'BBspeedCA'\n",
    "\n",
    "# weather and fire data to include in model\n",
    "rainint = True\n",
    "raintot = False\n",
    "high_t = True\n",
    "low_t = True\n",
    "humidity = True\n",
    "wind_speed = True\n",
    "wind_direction = True\n",
    "cloud_cover = False\n",
    "fire_direction = False\n",
    "fire_speed = False\n",
    "\n",
    "weather_variables = {\n",
    "    'rainint': rainint, \n",
    "    'raintot': raintot, \n",
    "    'High T': high_t, \n",
    "    'Low T': low_t, \n",
    "    'Humidity': humidity, \n",
    "    'Wind Speed': wind_speed, \n",
    "    'Wind Direction': wind_direction, \n",
    "    'Cloud Cover': cloud_cover,\n",
    "    'Fire Direction': fire_direction,\n",
    "    'Fire Speed': fire_speed\n",
    "}\n",
    "\n",
    "weather_vars = []\n",
    "\n",
    "for k, v in weather_variables.items():\n",
    "    if v == True:\n",
    "        weather_vars.append(k)\n",
    "\n",
    "#####################\n",
    "## HYPERPARAMETERS ##\n",
    "#####################\n",
    "\n",
    "# scale the weather data - yea or nay\n",
    "normalized_weather = True\n",
    "\n",
    "# the desired height and width (in pixels) of the matrix to feed into the CNN\n",
    "# 1 pixel side = 500 meters = 0.310686 miles\n",
    "matrix_dim = 32\n",
    "\n",
    "# multiplier for amount of zero-labeled data we want to add to dataset\n",
    "labeled_multiplier = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files from S3 to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_fire_data_from_s3(year, tif_directory):\n",
    "    '''\n",
    "    Pull files from S3 for the provided year and save to local directory\n",
    "    '''\n",
    "    \n",
    "    file_names = [\n",
    "        \"BBdayofburnCA.tif\",\n",
    "        \"BBdirectionCA.tif\",\n",
    "        \"BBfireid.tif\",\n",
    "        \"BBfirelineCA.tif\",\n",
    "        \"BBspeedCA.png\",\n",
    "        \"BBspeedCA.tif\",\n",
    "        \"ignitioncrop.pickle\",\n",
    "        \"polygoncrop.pickle\"\n",
    "    ]\n",
    "        \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    for f in file_names:\n",
    "        key = \"GlobalFire/\" + str(year) + \"/\" + f\n",
    "        path = '/home/ubuntu/wildfireplus/data/' + f\n",
    "        \n",
    "        s3.Bucket('hotzone').download_file(key, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_weather_data_from_s3(year, weather_directory):\n",
    "    '''\n",
    "    Pull files from S3 for the provided year and save to local directory\n",
    "    '''\n",
    "    \n",
    "    file_name = \"weather_data.pickle\"\n",
    "        \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    key = \"BayAreaWeather/\" + str(year) + \"/\" + file_name\n",
    "    path = '/home/ubuntu/wildfireplus/weather_data/' + file_name\n",
    "        \n",
    "    s3.Bucket('hotzone').download_file(key, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_weather_maxes_from_s3():\n",
    "    '''\n",
    "    Pull list of max weather values\n",
    "    '''\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    key = \"BayAreaWeather/max_values/max_values.pickle\"\n",
    "    directory = '/home/ubuntu/wildfireplus/weather_max/'\n",
    "    path = directory + 'max_values.pickle'\n",
    "    \n",
    "    s3.Bucket('hotzone').download_file(key, path)\n",
    "    \n",
    "    total_path = os.path.abspath(directory)\n",
    "    \n",
    "    for f in os.listdir(total_path):\n",
    "        if f.endswith('.pickle'):\n",
    "            max_values = total_path + '/' + f\n",
    "\n",
    "    max_values = pd.read_pickle(max_values)\n",
    "    \n",
    "    return max_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Dataset Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(directory):\n",
    "    '''\n",
    "    Process the dataset in the supplied directory and return matrices of which pixels belong to which fire and \n",
    "    which day of the year the pixel was on fire.\n",
    "    \n",
    "    Args: \n",
    "        - directory: name of directory with tif files\n",
    "    Returns: \n",
    "        - fire_data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    '''\n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "\n",
    "    tiff_files = []\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.tif'):\n",
    "            tiff_files.append(path + '/' + f)\n",
    "\n",
    "    tiff_dict = {}\n",
    "\n",
    "    # dictionary of tiff files\n",
    "    for f in tiff_files:\n",
    "        k = f.split('/')[-1].split('.tif')[0]\n",
    "        tiff_dict[k] = f\n",
    "\n",
    "    # convert to np array\n",
    "    fire_id = Image.open(tiff_dict['BBfireid'])\n",
    "    fire_id = np.array(fire_id)\n",
    "    fire_id[fire_id == -9999] = 0\n",
    "\n",
    "    fireline = Image.open(tiff_dict['BBfirelineCA'])\n",
    "    fireline = np.array(fireline)\n",
    "    fireline[fireline == -9999] = 0\n",
    "\n",
    "    # get list of unique fire_ids\n",
    "    fire_ids = set()\n",
    "\n",
    "    for row in fire_id:\n",
    "        for val in row:\n",
    "            fire_ids.add(val)\n",
    "\n",
    "    # remove 0 from fire_ids set because it does not denote a fire\n",
    "    fire_ids.remove(0)\n",
    "\n",
    "    # get dict with key value pairs of fire_id and an empty dict\n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        id = str(id)\n",
    "        fire_data_dict[id] = {}\n",
    "\n",
    "    for id in fire_ids:\n",
    "        indices = np.where(fire_id == id, 1, 0)\n",
    "        fire_data_dict[str(id)] = indices\n",
    "        \n",
    "    return fire_data_dict, fireline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_matrices(data_dict, fireline):\n",
    "    '''\n",
    "    Create matrices for each fire_id that show were the fire was on a given day during the year.\n",
    "    \n",
    "    Args:\n",
    "        - data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by that fire (0, 1)\n",
    "        - fireline: matrix denoting what day of year that pixel was on fire (1-365)\n",
    "    Returns:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    '''\n",
    "    \n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for key, val in data_dict.items():\n",
    "        data = {}\n",
    "                \n",
    "        for y in range(1, 366):\n",
    "            mask = ((fireline == y) & (val == 1))\n",
    "            mask = mask.astype(int)\n",
    "        \n",
    "            if np.sum(mask) > 0:\n",
    "                data[str(y)] = mask\n",
    "        \n",
    "        fire_data_dict[key] = data\n",
    "        \n",
    "    return fire_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_day_pairs(fire_data_dict):\n",
    "    '''\n",
    "    Create a list of sets where the first value is where the fire was on a given day and the second value is where\n",
    "    the fire was on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - fire_data_dict: a dictionary of the following structure:\n",
    "            {\n",
    "                \"fire_id\": {\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day,\n",
    "                    \"day_of_year\": one-hot encoded 2D array of fire spread on that day\n",
    "                }\n",
    "\n",
    "            }\n",
    "    Returns:\n",
    "        - train_labels: a list of sets where the first value of the set is a one-hot encoded 2D array of fire \n",
    "        spread on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2:\n",
    "        [\n",
    "            (one-hot encoded 2D array of fire spread on that day_1, one-hot encoded 2D array of fire spread on day_2),\n",
    "            (one-hot encoded 2D array of fire spread on that day_2, one-hot encoded 2D array of fire spread on day_3),\n",
    "        ]\n",
    "    '''\n",
    "    \n",
    "    train_labels = []\n",
    "\n",
    "    for key, value in fire_data_dict.items():\n",
    "        burn_matrices = list(value.values())\n",
    "        day_of_year = list(value.keys())\n",
    "        \n",
    "        for index, day in enumerate(burn_matrices):\n",
    "\n",
    "            if index < len(burn_matrices) - 1:\n",
    "                days = range(0, index+1)\n",
    "                \n",
    "                day_1 = 0\n",
    "                \n",
    "                for d in days:\n",
    "                    day_1 = np.add(day_1, burn_matrices[d])\n",
    "                \n",
    "                day_2_index = index + 1\n",
    "                day_2 = burn_matrices[day_2_index]\n",
    "                \n",
    "                doy = day_of_year[day_2_index]\n",
    "                \n",
    "                pair = (day_1, day_2)\n",
    "                train_labels.append((doy, pair))\n",
    "        \n",
    " \n",
    "    return train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fire_data_tiff(directory, file):\n",
    "    '''\n",
    "    Process the fire data in the supplied tiff file and return a dictionary of key day of year and value a matrix \n",
    "    making up the attribute of that tiff file\n",
    "\n",
    "    Args:\n",
    "        - directory: name of directory of supplemental data\n",
    "        - file: name of tiff file of supplemental data to add to model\n",
    "    Returns:\n",
    "        - fire_data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by the attribute of interest\n",
    "    '''\n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "\n",
    "    tiff_files = []\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.tif'):\n",
    "            tiff_files.append(path + '/' + f)\n",
    "\n",
    "    tiff_dict = {}\n",
    "\n",
    "    # dictionary of tiff files\n",
    "    for f in tiff_files:\n",
    "        k = f.split('/')[-1].split('.tif')[0]\n",
    "        tiff_dict[k] = f\n",
    "        \n",
    "    # convert day of burn tif to np array\n",
    "    fire_dob = Image.open(tiff_dict['BBdayofburnCA'])\n",
    "    fire_dob = np.array(fire_dob)\n",
    "    fire_dob[fire_dob == -9999] = 0\n",
    "\n",
    "    # convert tif of interest to np array\n",
    "    fire_data_mat = Image.open(tiff_dict[file])\n",
    "    fire_data_mat = np.array(fire_data_mat)\n",
    "    fire_data_mat[fire_data_mat == -9999] = 0\n",
    "    \n",
    "    # get list of unique days of burn\n",
    "    days_of_burn = list(np.unique(fire_dob))\n",
    "\n",
    "    # remove 0 from days of burn because it does not denote a fire\n",
    "    days_of_burn.remove(0)\n",
    "        \n",
    "    # get dict with key value pairs of fire_id and an empty dict\n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for idx in days_of_burn:\n",
    "        idx = int(idx)\n",
    "        \n",
    "        mask = (fire_dob == idx)        \n",
    "        mask = mask.astype(int)\n",
    "        \n",
    "        values = np.multiply(mask, fire_data_mat)\n",
    "        \n",
    "        idx = str(idx)\n",
    "        fire_data_dict[idx] = {}\n",
    "        fire_data_dict[idx]['Fire Direction'] = values \n",
    "\n",
    "    \n",
    "    return fire_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fire_data_png(directory, file):\n",
    "    '''\n",
    "    Process the fire data in the supplied png file and return a dictionary of key day of year and value a matrix \n",
    "    making up the attribute of that png file\n",
    "\n",
    "    Args:\n",
    "        - directory: name of directory of supplemental data\n",
    "        - file: name of png file of supplemental data to add to model\n",
    "    Returns:\n",
    "        - fire_data_dict: a dictionary where the key is \"fire_id\" and the value is a matrix of pixels \n",
    "        triggered by the attribute of interest\n",
    "    '''\n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "\n",
    "    tiff_files = []\n",
    "    png_files = []\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.tif'):\n",
    "            tiff_files.append(path + '/' + f)\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.png'):\n",
    "            png_files.append(path + '/' + f)\n",
    "    \n",
    "    tiff_dict = {}\n",
    "    png_dict = {}\n",
    "\n",
    "    # dictionary of tiff files\n",
    "    for f in tiff_files:\n",
    "        k = f.split('/')[-1].split('.tif')[0]\n",
    "        tiff_dict[k] = f\n",
    "    \n",
    "    for f in png_files:\n",
    "        k = f.split('/')[-1].split('.png')[0]\n",
    "        png_dict[k] = f\n",
    "        \n",
    "    # convert day of burn tif to np array\n",
    "    fire_dob = Image.open(tiff_dict['BBdayofburnCA'])\n",
    "    fire_dob = np.array(fire_dob)\n",
    "    fire_dob[fire_dob == -9999] = 0\n",
    "\n",
    "    # convert png of interest to np array\n",
    "    fire_data_mat = Image.open(png_dict[file])\n",
    "    fire_data_mat = np.array(fire_data_mat)\n",
    "    fire_data_mat[fire_data_mat == -9999] = 0\n",
    "    \n",
    "    # get list of unique days of burn\n",
    "    days_of_burn = list(np.unique(fire_dob))\n",
    "\n",
    "    # remove 0 from days of burn because it does not denote a fire\n",
    "    days_of_burn.remove(0)\n",
    "        \n",
    "    # get dict with key value pairs of fire_id and an empty dict\n",
    "    fire_data_dict = {}\n",
    "\n",
    "    for idx in days_of_burn:\n",
    "        idx = int(idx)\n",
    "        \n",
    "        mask = (fire_dob == idx)        \n",
    "        mask = mask.astype(int)\n",
    "        \n",
    "        values = np.multiply(mask, fire_data_mat)\n",
    "        \n",
    "        idx = str(idx)\n",
    "        fire_data_dict[idx] = {}\n",
    "        fire_data_dict[idx]['Fire Speed'] = values \n",
    "\n",
    "    \n",
    "    return fire_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(dict_1, dict_2):\n",
    "    '''\n",
    "    A helper function to combine the values of two dictionaries that have the same keys.\n",
    "    \n",
    "    Args:\n",
    "        - dict_1: a dictionary of key day of year, and value a dictionary of key fire attribute and value a matrix\n",
    "        denoting where that attribute is triggered\n",
    "        - dict_2: a dictionary of key day of year, and value a dictionary of key fire attribute and value a matrix\n",
    "        denoting where that attribute is triggered\n",
    "    Returns:\n",
    "        - dict_2: combined dictionary of dict_1 and dict_2\n",
    "    '''\n",
    "      \n",
    "    for k, v in dict_1.items():\n",
    "        for att, mat in v.items():\n",
    "            dict_2[k][att] = mat\n",
    "            \n",
    "    return dict_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_dict(directory, normalized_weather, weather_vars, fire_data_dict):\n",
    "    '''\n",
    "    Create a dictionary of weather data from a pickled file\n",
    "    Args:\n",
    "        - directory: path to weather pickle file\n",
    "        - normalized_weather: True/False to scale using max value\n",
    "        - weather_vars: list of weather variables to include in model\n",
    "    Returns:\n",
    "        - weather_data: dictionary of key (day of year) and value (dictionary of key (weather parameter) \n",
    "        and value (matrix of value for each pixel))\n",
    "        - max_values: a list of max values for each weather feature to use to normalize data\n",
    "    '''\n",
    "\n",
    "    path = os.path.abspath(directory)\n",
    "    \n",
    "    weather_file = ''\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.pickle'):\n",
    "            weather_file = path + '/' + f\n",
    "    \n",
    "    weather = pd.read_pickle(weather_file)\n",
    "    \n",
    "    weather_dict = {}\n",
    "    \n",
    "    for k, v in weather.items():\n",
    "        weather_dict[k] = {}\n",
    "        \n",
    "        for att, matrix in v.items():\n",
    "            if att in weather_vars:\n",
    "                \n",
    "                # scale to kelvin\n",
    "                if att in ['High T', 'Low T']:\n",
    "                    mat = np.nan_to_num(matrix)\n",
    "                    mat += 273.15\n",
    "                    weather_dict[k][att] = mat\n",
    "                else:\n",
    "                    mat = np.nan_to_num(matrix)\n",
    "                    weather_dict[k][att] = mat\n",
    "     \n",
    "    weather_data = {}\n",
    "\n",
    "    for k, v in weather_dict.items():\n",
    "        doy = dt.strptime(k, \"%Y-%m-%d\").strftime(\"%-j\")\n",
    "        weather_data[doy] = v\n",
    "            \n",
    "    # scale weather data \n",
    "    max_values = pull_weather_maxes_from_s3()\n",
    "    \n",
    "    return weather_data, max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(max_values, normalized_weather, day_of_year, x, y):\n",
    "    '''\n",
    "    Fetch weather data for the relevant day and pixel.\n",
    "    \n",
    "    Args:\n",
    "        - max_values: list of max_values for each weather features\n",
    "        - normalized_weather: whether the weather data should be normalized - true/false\n",
    "        - day_of_year: day of the year (1-365)\n",
    "        - x: x-coordinate of matrix\n",
    "        - y: y-coordinate of matrix\n",
    "    Returns:\n",
    "        - weather_list: an array of relevant weather data for that pixel\n",
    "    '''\n",
    "    \n",
    "    weather_list = []\n",
    "    \n",
    "    day_weather = weather_data.get(day_of_year)\n",
    "\n",
    "    if day_weather is None:\n",
    "        return None\n",
    "    else:\n",
    "        for k, v in day_weather.items():\n",
    "            if normalized_weather == True:\n",
    "                max_val = max_values.get(k, 1)\n",
    "                \n",
    "                try:\n",
    "                    val = v[x,y]\n",
    "                    value = val/max_val\n",
    "                    \n",
    "                    if math.isnan(value):\n",
    "                        weather_list.append(0)\n",
    "                    else:\n",
    "                        weather_list.append(value)\n",
    "                except IndexError:\n",
    "                    return None\n",
    "            else:\n",
    "                try:\n",
    "                    weather_list.append(v[x,y])\n",
    "                except IndexError:\n",
    "                    return None\n",
    "    \n",
    "    return weather_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Dataset for CNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset, matrix_dim, data_len, side):\n",
    "    '''\n",
    "    Supplement the list produced in `create_labeled_data` with data where there was no data\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "        - data_len: how many \"no-fire\" pixel-matrix pairs we want to return\n",
    "        - side: half the length of the dimension of the outpur matrix\n",
    "    Returns:\n",
    "        - no_fire: a list of sets, where the second value (0, 1) represents whether fire is present for a given \n",
    "        pixel, and the first value is a matrix centered on the second value for the previous day and represents \n",
    "        where the fire was on the previous day\n",
    "    '''\n",
    "        \n",
    "    no_fire = []\n",
    "    vals = []\n",
    "    \n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 0)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "    vals = random.sample(vals, 2*data_len)\n",
    "    \n",
    "    for (xi, yi) in vals:\n",
    "        xi_r = xi + side\n",
    "        xi_l = xi - side\n",
    "        yi_b = yi + side\n",
    "        yi_t = yi - side\n",
    "\n",
    "        m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "\n",
    "        # control for edge cases where shape doesn't match up - not sure why this is happening\n",
    "        if m.shape == (matrix_dim, matrix_dim):\n",
    "            weather_data = fetch_weather_data(max_values, normalized_weather, doy, xi, yi)\n",
    "            if weather_data is not None:\n",
    "                no_fire.append(((weather_data, m), 0))\n",
    "    \n",
    "    len_no_fire = len(no_fire)\n",
    "    \n",
    "    num_pixels = min(len_no_fire, data_len)\n",
    "    no_fire = random.sample(no_fire, data_len)\n",
    "    \n",
    "    return no_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_data(dataset, matrix_dim, labeled_multiplier):\n",
    "    '''\n",
    "    Create a list of sets where the first value is a matrix of pixels on a given day and the second value denotes\n",
    "    whether there was fire in the center pixel on the following day.\n",
    "    \n",
    "    Args:\n",
    "        - dataset: a list of sets where the first value of the set is a one-hot encoded 2D array of fire spread \n",
    "        on day_1 and the second value of the set is a one-hot encoded 2D array of fire spread on day_2\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "        - labeled_multiplier: a hyperparameter for how much \"no-fire\" labeled data to add to the training set\n",
    "    Returns:\n",
    "        - data: a list of sets, where the second value (0, 1) represents whether fire is present for a given pixel, \n",
    "        and the first value is a matrix centered on the second value for the previous day and represents where the \n",
    "        fire was on the previous day\n",
    "    '''\n",
    "\n",
    "    side = int(matrix_dim/2)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for (doy, (x, y)) in dataset:    \n",
    "\n",
    "        x = np.pad(x, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "        y = np.pad(y, pad_width=matrix_dim, mode='constant', constant_values=0)\n",
    "\n",
    "        vals = np.where(y == 1)\n",
    "        vals = list(zip(vals[0], vals[1]))\n",
    "\n",
    "        for (xi, yi) in vals:\n",
    "            xi_r = xi + side\n",
    "            xi_l = xi - side\n",
    "            yi_b = yi + side\n",
    "            yi_t = yi - side\n",
    "\n",
    "            m = x[xi_l:xi_r, yi_t:yi_b]\n",
    "                        \n",
    "            weather_data = fetch_weather_data(max_values, normalized_weather, doy, xi, yi)\n",
    "            \n",
    "            if weather_data is not None:\n",
    "                data.append(((weather_data, m), 1))\n",
    "    \n",
    "    data_len = len(data)*labeled_multiplier\n",
    "    \n",
    "    # balance this dataset with values where there is no fire\n",
    "    print('Balance dataset')\n",
    "    no_fire = balance_dataset(dataset, matrix_dim, data_len, side)\n",
    "    \n",
    "    # combine and shuffle\n",
    "    data += no_fire    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset_for_non_cnn(data, matrix_dim):\n",
    "    '''\n",
    "    Takes a list of ((weather_data, fire_data), integer) pairs and returns fire data, weather data, and output labels.\n",
    "    \n",
    "    Args:\n",
    "        - data: a list of (matrix, integer) pairs\n",
    "        - matrix_dim: a hyperparameter for the height and width of the matrices fed into the CNN\n",
    "    Returns:\n",
    "        - fire: array of input data in matrix_dim X matrix_dim shape\n",
    "        - weather: list of normalized weather weights\n",
    "        - Y: array of output labels (0 or 1)\n",
    "    '''\n",
    "        \n",
    "    Y = []\n",
    "    x = []\n",
    "\n",
    "    for ((w, f), y) in data:        \n",
    "        \n",
    "        f = [f.sum()**2]\n",
    "        w += f\n",
    "        \n",
    "        x.append(w)\n",
    "        Y.append(y)\n",
    "                \n",
    "    Y = np.asarray(Y)\n",
    "    x = np.asarray(x)\n",
    "     \n",
    "    return x, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Starting data preprocessing for year:  2003\n",
      "2020-04-19 16:37:10.207846\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:05:02.173268\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2004\n",
      "2020-04-19 16:42:12.381854\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:04:07.670491\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2005\n",
      "2020-04-19 16:46:20.052584\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:03:02.519480\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2006\n",
      "2020-04-19 16:49:22.573109\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:03:18.710259\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2007\n",
      "2020-04-19 16:52:41.284469\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:02:16.450676\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2008\n",
      "2020-04-19 16:54:57.736455\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:07:59.228835\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2009\n",
      "2020-04-19 17:02:56.983373\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:06:49.204138\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2010\n",
      "2020-04-19 17:09:46.188224\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:02:07.183770\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2011\n",
      "2020-04-19 17:11:53.372825\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:02:10.870283\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2012\n",
      "2020-04-19 17:14:04.243303\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:01:40.927322\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2013\n",
      "2020-04-19 17:15:45.171528\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:02:44.559871\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2014\n",
      "2020-04-19 17:18:29.731611\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:01:51.226305\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2015\n",
      "2020-04-19 17:20:20.958139\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:03:01.897506\n",
      "*****************************************\n",
      "Starting data preprocessing for year:  2016\n",
      "2020-04-19 17:23:22.856352\n",
      "*****************************************\n",
      "Balance dataset\n",
      "Time:  0:02:58.142708\n"
     ]
    }
   ],
   "source": [
    "# run the datat preprocessing pipeline\n",
    "\n",
    "years = [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "train_x = []\n",
    "label_y = []\n",
    "\n",
    "for y in years:\n",
    "    print('*****************************************')\n",
    "    print('Starting data preprocessing for year: ', y)\n",
    "    time = dt.now()\n",
    "    print (time)\n",
    "    print('*****************************************')\n",
    "    \n",
    "    # pull data from s3\n",
    "    pull_fire_data_from_s3(y, tif_directory)\n",
    "    pull_weather_data_from_s3(y, weather_directory)\n",
    "\n",
    "    # get fire speed data\n",
    "    fire_speed_data_dict = process_fire_data_png(tif_directory, speed_file)\n",
    "\n",
    "    # get fire direction data\n",
    "    fire_dir_data_dict = process_fire_data_tiff(tif_directory, direction_file)\n",
    "\n",
    "    # combine fire speed and fire direction datasets\n",
    "    fire_data_dict = combine_dicts(fire_speed_data_dict, fire_dir_data_dict)\n",
    "\n",
    "    # create weather dict and combine with fire speed and fire direction\n",
    "    weather_data, max_values = create_weather_dict(weather_directory, normalized_weather, weather_vars, fire_data_dict)\n",
    "    \n",
    "    # return matrices of which pixels belong to which fire and which day of the year the pixel was on fire\n",
    "    fire_data_dict, fireline = data_processing(tif_directory)\n",
    "\n",
    "    # create matrices for each fire_id that show were the fire was on a given day during the year \n",
    "    fire_data_dict = create_one_hot_matrices(fire_data_dict, fireline)\n",
    "\n",
    "    # create a list of sets where the first value is where the fire was on a given day and the second value is where\n",
    "    # the fire was on the following day\n",
    "    small_dataset = create_day_pairs(fire_data_dict)\n",
    "\n",
    "    # create a list of sets where the first value is a matrix of pixels on a given day and the second value denotes\n",
    "    # whether there was fire in the center pixel on the following day\n",
    "    data = create_labeled_data(small_dataset, matrix_dim, labeled_multiplier)\n",
    "\n",
    "    # takes data pairs and returns fire data, weather data, and output labels\n",
    "    x, Y = prep_dataset_for_non_cnn(data, matrix_dim)\n",
    "    train_x.append(x)\n",
    "    label_y.append(Y)\n",
    "        \n",
    "    print('Time: ', (dt.now() - time))\n",
    "\n",
    "# unnest numpy arrays\n",
    "flat_Y = []\n",
    "for sublist in label_y:\n",
    "    for item in sublist:\n",
    "        flat_Y.append(np.asarray(item))\n",
    "\n",
    "label_y = flat_Y\n",
    "\n",
    "flat_X = []\n",
    "for sublist in train_x:\n",
    "    for item in sublist:\n",
    "        flat_X.append(np.asarray(item))\n",
    "        \n",
    "train_x = flat_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Non-CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-CNN Model #1: Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier Accuracy:  0.6809444021325209\n",
      "Random Classifier Recall:  0.19813404417364813\n",
      "Random Classifier F1 Score:  0.19777629953435333\n",
      "Random Classifier AUC Score:  0.4983815689261234\n"
     ]
    }
   ],
   "source": [
    "# fit random classifier model\n",
    "random = DummyClassifier()\n",
    "random.fit(train_x, label_y)\n",
    "\n",
    "# make predictions\n",
    "random_pred = random.predict(train_x)\n",
    "\n",
    "# measure accuracy\n",
    "random_acc = random.score(train_x, label_y)\n",
    "\n",
    "# measure recall\n",
    "random_recall = sklearn.metrics.recall_score(label_y, random_pred)\n",
    "\n",
    "# measure f1 score\n",
    "random_f1score = sklearn.metrics.f1_score(label_y, random_pred)\n",
    "\n",
    "# measure auc score\n",
    "random_aucscore = sklearn.metrics.roc_auc_score(label_y, random_pred)\n",
    "\n",
    "\n",
    "print('Random Classifier Accuracy: ', random_acc)\n",
    "print('Random Classifier Recall: ', random_recall)\n",
    "print('Random Classifier F1 Score: ', random_f1score)\n",
    "print('Random Classifier AUC Score: ', random_aucscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-CNN Model #2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.9528332063975629\n",
      "Logistic Regression Recall:  0.7662985529322163\n",
      "Logistic Regression F1 Score:  0.8666422618919442\n",
      "Logistic Regression AUC Score:  0.882882711348058\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(train_x, label_y)\n",
    "\n",
    "# make predictions\n",
    "log_reg_pred = log_reg.predict(train_x)\n",
    "\n",
    "# measure accuracy\n",
    "log_reg_acc = log_reg.score(train_x, label_y)\n",
    "\n",
    "# measure recall\n",
    "log_reg_recall = sklearn.metrics.recall_score(label_y, log_reg_pred)\n",
    "\n",
    "# measure f1 score\n",
    "log_reg_f1score = sklearn.metrics.f1_score(label_y, log_reg_pred)\n",
    "\n",
    "# measure auc score\n",
    "log_reg_aucscore = sklearn.metrics.roc_auc_score(label_y, log_reg_pred)\n",
    "\n",
    "\n",
    "print('Logistic Regression Accuracy: ', log_reg_acc)\n",
    "print('Logistic Regression Recall: ', log_reg_recall)\n",
    "print('Logistic Regression F1 Score: ', log_reg_f1score)\n",
    "print('Logistic Regression AUC Score: ', log_reg_aucscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-CNN Model #3: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Accuracy:  0.932955064737243\n",
      "Gaussian Naive Bayes Recall:  0.6659177456207159\n",
      "Gaussian Naive Bayes F1 Score:  0.7989126710373028\n",
      "Gaussian Naive Bayes AUC Score:  0.8328160700685453\n"
     ]
    }
   ],
   "source": [
    "# fit gaussian naive bayes model\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb.fit(train_x, label_y)\n",
    "\n",
    "# make predictions\n",
    "gnb_pred = gaussian_nb.predict(train_x)\n",
    "\n",
    "# measure accuracy\n",
    "gnb_acc = gaussian_nb.score(train_x, label_y)\n",
    "\n",
    "# measure recall\n",
    "gnb_recall = sklearn.metrics.recall_score(label_y, gnb_pred)\n",
    "\n",
    "# measure f1 score\n",
    "gnb_f1score = sklearn.metrics.f1_score(label_y, gnb_pred)\n",
    "\n",
    "# measure auc score\n",
    "gnb_aucscore = sklearn.metrics.roc_auc_score(label_y, gnb_pred)\n",
    "\n",
    "\n",
    "print('Gaussian Naive Bayes Accuracy: ', gnb_acc)\n",
    "print('Gaussian Naive Bayes Recall: ', gnb_recall)\n",
    "print('Gaussian Naive Bayes F1 Score: ', gnb_f1score)\n",
    "print('Gaussian Naive Bayes AUC Score: ', gnb_aucscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
